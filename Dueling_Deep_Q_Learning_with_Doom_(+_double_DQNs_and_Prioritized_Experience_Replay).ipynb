{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4OL2poCFQ3h"
   },
   "source": [
    "# DDDQN  (Double Dueling Deep Q Learning with Prioritized Experience Replay)  DoomüïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Dueling Double Deep Q learning architecture with Prioritized Experience Replay.</b> <br>\n",
    "\n",
    "Our agent playing Doom after 3 hours of training of **CPU**, remember that our agent needs about 2 days of **GPU** to have optimal score, we'll train from beginning to end the most important architectures (PPO with transfer):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/projects/doomdeathmatc.gif\" alt=\"Doom Deathmatch\"/>\n",
    "\n",
    "But we can see that our agent **understand that he needs to kill enemies before being able to move forward (if he moves forward without killing ennemies he will be killed before getting the vest)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2a4H7klFQ3k"
   },
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97NxCg7QFQ3m"
   },
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- Improvments in Deep Q-learning [Article]()\n",
    "- You can follow this notebook using my [video tutorial](https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7NuU2TzFQ3n",
    "outputId": "432d1ec2-d6ff-44fe-8735-9a1966b59731"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWFi5tS-FQ3t"
   },
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ab_XWXlFQ3u"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpVSALR7FQ3y"
   },
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 7 possible actions: turn left, turn right, move left, move right, shoot (attack)...`[[0,0,0,0,1]...]` so we don't need to do one hot encoding (thanks to <a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out). \n",
    "\n",
    "### Our environment\n",
    "<img src=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/assets/img/video%20projects/deadlycorridor.png\" style=\"max-width:500px;\" alt=\"Vizdoom deadly corridor\"/>\n",
    "\n",
    "The purpose of this scenario is to teach the agent to navigate towards his fundamental goal (the vest) and make sure he survives at the same time.\n",
    "\n",
    "- Map is a corridor with shooting monsters on both sides (6 monsters in total). \n",
    "- A green vest is placed at the oposite end of the corridor. \n",
    "- **Reward is proportional (negative or positive) to change of the distance between the player and the vest.** \n",
    "- If player ignores monsters on the sides and runs straight for the vest he will be killed somewhere along the way. \n",
    "- To ensure this behavior doom_skill = 5 (config) is needed.\n",
    "\n",
    "<br>\n",
    "REWARDS:\n",
    "\n",
    "- +dX for getting closer to the vest. -dX for getting further from the vest.\n",
    "- death penalty = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "se4CcfVwFQ3z"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"/home/schubert/anaconda3/envs/DL/lib/python3.6/site-packages/vizdoom/scenarios/deadly_corridor.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"/home/schubert/anaconda3/envs/DL/lib/python3.6/site-packages/vizdoom/scenarios/deadly_corridor.wad\")\n",
    "    \n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (5 possible actions)\n",
    "    # possible_actions = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]...]\n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImptodLZFQ33"
   },
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvDdn_6yFQ37"
   },
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8t8cIhgFQ38"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    frame = np.mean(frame,0)\n",
    "    cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [75,90])\n",
    "    plt.imshow(preprocessed_frame)\n",
    "    \n",
    "    \n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiQ33QxPFQ3_"
   },
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snuG3bOBFQ4A"
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpd32cKiFQ4E"
   },
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OZTSZzhFQ4G"
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 7 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000         # Total episodes for training\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS \n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 10000   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 100000       # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvV4fZiqFQ4K"
   },
   "source": [
    "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model (aka DDDQN) üß†\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
    "This is our Dueling Double Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Then it is passed through 2 streams\n",
    "    - One that calculates V(s)\n",
    "    - The other that calculates A(s,a)\n",
    "- Finally an agregating layer\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPnXcVXOFQ4M"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" \n",
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            \n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value_fc\")\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value\")\n",
    "            \n",
    "            # The one that calculate A(s,a)\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantages\")\n",
    "            \n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "384YVOaSFQ4R"
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuQIHDUBFQ4V"
   },
   "source": [
    "## Step 6: Prioritized Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Prioritized Experience Replay method.** <br>\n",
    "\n",
    "As explained in the article, **we can't use a simple array to do that because sampling from it will be not efficient, so we use a binary tree data type (in a binary tree each node has no + than 2 children).** More precisely, a sumtree, which is a binary tree where parents nodes are the sum of the children nodes.\n",
    "\n",
    "If you don't know what is a binary tree check this awesome video https://www.youtube.com/watch?v=oSWTXtMglKE\n",
    "\n",
    "\n",
    "This SumTree implementation was taken from Morvan Zhou in his chinese course about Reinforcement Learning\n",
    "\n",
    "To summarize:\n",
    "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
    "    <br><br>\n",
    "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
    "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
    "    - **def update**: we update the leaf priority score and propagate through tree.\n",
    "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
    "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
    "<br><br>\n",
    "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
    "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
    "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
    "    - **def sample**:\n",
    "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "         - Then a value is uniformly sampled from each range\n",
    "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "         - Then, we calculate IS weights for each minibatch element\n",
    "    - **def update_batch**: update the priorities on the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3XPYiozFQ4Y"
   },
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsmZy0hyFQ4b"
   },
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtN54Im3FQ4c"
   },
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1oOAp9wFQ4f"
   },
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvB-fkc6FQ4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.init()\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    clear_output()\n",
    "    print(i)\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        break\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbtWDEznFQ4j"
   },
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ0IB_NDFQ4j"
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"tensorboard/dddqn\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM7x7dZEFQ4m"
   },
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights for DQN\n",
    "* Initialize target value weights w- <- w\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        \n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * Every C steps, reset: $w^- \\leftarrow w$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75BMTrkRFQ4n"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3P_8N7tCFQ4r"
   },
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A40XrfsUFQ4u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Episode: 0 Total reward: -108.68434143066406 Training loss: 0.0000 Explore P: 0.9933\n",
      "Model Saved\n",
      "Episode: 1 Total reward: -115.19001770019531 Training loss: 0.0000 Explore P: 0.9892\n",
      "Episode: 2 Total reward: -105.44499206542969 Training loss: 0.0000 Explore P: 0.9855\n",
      "Episode: 3 Total reward: -93.06541442871094 Training loss: 0.0000 Explore P: 0.9800\n",
      "Episode: 4 Total reward: -100.5716552734375 Training loss: 0.0000 Explore P: 0.9759\n",
      "Episode: 5 Total reward: -115.99882507324219 Training loss: 0.0000 Explore P: 0.9704\n",
      "Model Saved\n",
      "Episode: 6 Total reward: -101.0218505859375 Training loss: 0.0000 Explore P: 0.9665\n",
      "Episode: 7 Total reward: -73.44865417480469 Training loss: 0.0000 Explore P: 0.9642\n",
      "Episode: 8 Total reward: -115.96614074707031 Training loss: 0.0000 Explore P: 0.9592\n",
      "Episode: 9 Total reward: -97.9586181640625 Training loss: 0.0000 Explore P: 0.9569\n",
      "Episode: 10 Total reward: -108.1531982421875 Training loss: 0.0000 Explore P: 0.9529\n",
      "Model Saved\n",
      "Episode: 11 Total reward: -94.34249877929688 Training loss: 0.0000 Explore P: 0.9465\n",
      "Episode: 12 Total reward: -72.02243041992188 Training loss: 0.0000 Explore P: 0.9413\n",
      "Episode: 13 Total reward: -111.26121520996094 Training loss: 0.0000 Explore P: 0.9377\n",
      "Episode: 14 Total reward: -68.92214965820312 Training loss: 0.0000 Explore P: 0.9341\n",
      "Episode: 15 Total reward: -101.33335876464844 Training loss: 0.0000 Explore P: 0.9321\n",
      "Model Saved\n",
      "Episode: 16 Total reward: -101.82794189453125 Training loss: 0.0000 Explore P: 0.9271\n",
      "Episode: 17 Total reward: -103.17181396484375 Training loss: 0.0000 Explore P: 0.9252\n",
      "Episode: 18 Total reward: -101.15440368652344 Training loss: 0.0000 Explore P: 0.9206\n",
      "Episode: 19 Total reward: -46.49162292480469 Training loss: 0.0000 Explore P: 0.9117\n",
      "Episode: 20 Total reward: -106.26618957519531 Training loss: 0.0000 Explore P: 0.9064\n",
      "Model Saved\n",
      "Episode: 21 Total reward: -88.10983276367188 Training loss: 0.0000 Explore P: 0.9026\n",
      "Episode: 22 Total reward: -93.84300231933594 Training loss: 0.0000 Explore P: 0.8989\n",
      "Episode: 23 Total reward: -30.472427368164062 Training loss: 0.0000 Explore P: 0.8931\n",
      "Episode: 24 Total reward: -111.77365112304688 Training loss: 0.0000 Explore P: 0.8897\n",
      "Episode: 25 Total reward: -59.88533020019531 Training loss: 0.0000 Explore P: 0.8874\n",
      "Model Saved\n",
      "Episode: 26 Total reward: -96.11247253417969 Training loss: 0.0000 Explore P: 0.8853\n",
      "Episode: 27 Total reward: -91.09608459472656 Training loss: 0.0000 Explore P: 0.8818\n",
      "Episode: 28 Total reward: -114.25064086914062 Training loss: 0.0000 Explore P: 0.8772\n",
      "Episode: 29 Total reward: -84.32461547851562 Training loss: 0.0000 Explore P: 0.8729\n",
      "Episode: 30 Total reward: 15.411773681640625 Training loss: 0.0000 Explore P: 0.8645\n",
      "Model Saved\n",
      "Episode: 31 Total reward: -101.87397766113281 Training loss: 0.0000 Explore P: 0.8624\n",
      "Episode: 32 Total reward: -70.09844970703125 Training loss: 0.0000 Explore P: 0.8577\n",
      "Episode: 33 Total reward: -115.45458984375 Training loss: 0.0000 Explore P: 0.8543\n",
      "Episode: 34 Total reward: -38.18870544433594 Training loss: 0.0000 Explore P: 0.8497\n",
      "Episode: 35 Total reward: -44.62908935546875 Training loss: 0.0000 Explore P: 0.8439\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -38.55494689941406 Training loss: 0.0000 Explore P: 0.8392\n",
      "Episode: 37 Total reward: -85.87747192382812 Training loss: 0.0000 Explore P: 0.8371\n",
      "Episode: 38 Total reward: -81.97860717773438 Training loss: 0.0000 Explore P: 0.8341\n",
      "Episode: 39 Total reward: -7.6401824951171875 Training loss: 0.0000 Explore P: 0.8286\n",
      "Episode: 40 Total reward: -74.712890625 Training loss: 0.0000 Explore P: 0.8255\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -88.4813232421875 Training loss: 0.0000 Explore P: 0.8222\n",
      "Episode: 42 Total reward: -115.97361755371094 Training loss: 0.0000 Explore P: 0.8177\n",
      "Episode: 43 Total reward: -53.809967041015625 Training loss: 0.0000 Explore P: 0.8143\n",
      "Episode: 44 Total reward: 15.202682495117188 Training loss: 0.0000 Explore P: 0.8088\n",
      "Episode: 45 Total reward: -109.55795288085938 Training loss: 0.0000 Explore P: 0.8069\n",
      "Model Saved\n",
      "Episode: 46 Total reward: -39.9342041015625 Training loss: 0.0000 Explore P: 0.8038\n",
      "Episode: 47 Total reward: -77.84223937988281 Training loss: 0.0000 Explore P: 0.8021\n",
      "Episode: 48 Total reward: -78.64805603027344 Training loss: 0.0000 Explore P: 0.8004\n",
      "Episode: 49 Total reward: -53.20921325683594 Training loss: 0.0000 Explore P: 0.7946\n",
      "Episode: 50 Total reward: -29.935943603515625 Training loss: 0.0000 Explore P: 0.7915\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -60.20054626464844 Training loss: 0.0000 Explore P: 0.7898\n",
      "Episode: 52 Total reward: -74.98194885253906 Training loss: 0.0000 Explore P: 0.7867\n",
      "Episode: 53 Total reward: -7.010284423828125 Training loss: 0.0000 Explore P: 0.7825\n",
      "Episode: 54 Total reward: -101.52871704101562 Training loss: 0.0000 Explore P: 0.7783\n",
      "Episode: 55 Total reward: -71.521728515625 Training loss: 0.0000 Explore P: 0.7751\n",
      "Model Saved\n",
      "Episode: 56 Total reward: -53.812591552734375 Training loss: 0.0000 Explore P: 0.7731\n",
      "Episode: 57 Total reward: -83.40748596191406 Training loss: 0.0000 Explore P: 0.7702\n",
      "Episode: 58 Total reward: -6.5820770263671875 Training loss: 0.0000 Explore P: 0.7670\n",
      "Episode: 59 Total reward: -60.167327880859375 Training loss: 0.0000 Explore P: 0.7625\n",
      "Episode: 60 Total reward: -98.53024291992188 Training loss: 0.0000 Explore P: 0.7608\n",
      "Model Saved\n",
      "Episode: 61 Total reward: -70.1495361328125 Training loss: 0.0000 Explore P: 0.7589\n",
      "Episode: 62 Total reward: -15.5286865234375 Training loss: 0.0000 Explore P: 0.7561\n",
      "Episode: 63 Total reward: -76.812744140625 Training loss: 0.0000 Explore P: 0.7519\n",
      "Episode: 64 Total reward: -12.447357177734375 Training loss: 0.0000 Explore P: 0.7478\n",
      "Episode: 65 Total reward: -52.05718994140625 Training loss: 0.0000 Explore P: 0.7451\n",
      "Model Saved\n",
      "Episode: 66 Total reward: -80.98321533203125 Training loss: 0.0000 Explore P: 0.7423\n",
      "Episode: 67 Total reward: -36.59869384765625 Training loss: 0.0000 Explore P: 0.7380\n",
      "Episode: 68 Total reward: -75.53147888183594 Training loss: 0.0000 Explore P: 0.7352\n",
      "Episode: 69 Total reward: -75.64892578125 Training loss: 0.0000 Explore P: 0.7312\n",
      "Episode: 70 Total reward: -54.516357421875 Training loss: 0.0000 Explore P: 0.7285\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -4.42279052734375 Training loss: 0.0000 Explore P: 0.7257\n",
      "Episode: 72 Total reward: -3.3126678466796875 Training loss: 0.0000 Explore P: 0.7218\n",
      "Episode: 73 Total reward: -51.23081970214844 Training loss: 0.0000 Explore P: 0.7181\n",
      "Episode: 74 Total reward: -50.48712158203125 Training loss: 0.0000 Explore P: 0.7140\n",
      "Episode: 75 Total reward: -25.886245727539062 Training loss: 0.0000 Explore P: 0.7111\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 10.12335205078125 Training loss: 0.0000 Explore P: 0.7082\n",
      "Episode: 77 Total reward: -6.5252685546875 Training loss: 0.0000 Explore P: 0.7053\n",
      "Episode: 78 Total reward: -66.72145080566406 Training loss: 0.0000 Explore P: 0.7025\n",
      "Episode: 79 Total reward: -33.23394775390625 Training loss: 0.0000 Explore P: 0.7007\n",
      "Episode: 80 Total reward: -58.78337097167969 Training loss: 0.0000 Explore P: 0.6992\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -75.32537841796875 Training loss: 0.0000 Explore P: 0.6975\n",
      "Episode: 82 Total reward: 287.6817321777344 Training loss: 0.0000 Explore P: 0.6920\n",
      "Episode: 83 Total reward: 6.2388916015625 Training loss: 0.0000 Explore P: 0.6891\n",
      "Episode: 84 Total reward: 29.924530029296875 Training loss: 0.0000 Explore P: 0.6862\n",
      "Episode: 85 Total reward: -101.87744140625 Training loss: 0.0000 Explore P: 0.6846\n",
      "Model Saved\n",
      "Episode: 86 Total reward: -46.03791809082031 Training loss: 0.0000 Explore P: 0.6819\n",
      "Episode: 87 Total reward: -62.58897399902344 Training loss: 0.0000 Explore P: 0.6783\n",
      "Episode: 88 Total reward: -32.64031982421875 Training loss: 0.0000 Explore P: 0.6754\n",
      "Episode: 89 Total reward: -94.96556091308594 Training loss: 0.0000 Explore P: 0.6737\n",
      "Episode: 90 Total reward: -35.490081787109375 Training loss: 0.0000 Explore P: 0.6711\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -78.79286193847656 Training loss: 0.0000 Explore P: 0.6694\n",
      "Episode: 92 Total reward: -16.185699462890625 Training loss: 0.0000 Explore P: 0.6668\n",
      "Episode: 93 Total reward: -3.0313873291015625 Training loss: 0.0000 Explore P: 0.6642\n",
      "Episode: 94 Total reward: -19.308380126953125 Training loss: 0.0000 Explore P: 0.6617\n",
      "Episode: 95 Total reward: 60.603485107421875 Training loss: 0.0000 Explore P: 0.6589\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -74.72946166992188 Training loss: 0.0000 Explore P: 0.6574\n",
      "Episode: 97 Total reward: -14.44390869140625 Training loss: 0.0000 Explore P: 0.6549\n",
      "Episode: 98 Total reward: -51.492218017578125 Training loss: 0.0000 Explore P: 0.6523\n",
      "Episode: 99 Total reward: -40.88508605957031 Training loss: 0.0000 Explore P: 0.6498\n",
      "Episode: 100 Total reward: -49.89707946777344 Training loss: 0.0000 Explore P: 0.6484\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 37.45831298828125 Training loss: 0.0000 Explore P: 0.6451\n",
      "Episode: 102 Total reward: -66.16386413574219 Training loss: 0.0000 Explore P: 0.6425\n",
      "Episode: 103 Total reward: -84.42118835449219 Training loss: 0.0000 Explore P: 0.6411\n",
      "Episode: 104 Total reward: -28.354461669921875 Training loss: 0.0000 Explore P: 0.6394\n",
      "Episode: 105 Total reward: -80.61537170410156 Training loss: 0.0000 Explore P: 0.6380\n",
      "Model Saved\n",
      "Episode: 106 Total reward: -34.078216552734375 Training loss: 0.0000 Explore P: 0.6363\n",
      "Episode: 107 Total reward: -43.67948913574219 Training loss: 0.0000 Explore P: 0.6331\n",
      "Episode: 108 Total reward: -80.4974365234375 Training loss: 0.0000 Explore P: 0.6315\n",
      "Episode: 109 Total reward: -100.72630310058594 Training loss: 0.0000 Explore P: 0.6303\n",
      "Episode: 110 Total reward: -13.728500366210938 Training loss: 0.0000 Explore P: 0.6279\n",
      "Model Saved\n",
      "Episode: 111 Total reward: -58.62005615234375 Training loss: 0.0000 Explore P: 0.6255\n",
      "Episode: 112 Total reward: -3.5663909912109375 Training loss: 0.0000 Explore P: 0.6231\n",
      "Episode: 113 Total reward: -82.763916015625 Training loss: 0.0000 Explore P: 0.6217\n",
      "Episode: 114 Total reward: -1.986358642578125 Training loss: 0.0000 Explore P: 0.6193\n",
      "Episode: 115 Total reward: -46.67710876464844 Training loss: 0.0000 Explore P: 0.6169\n",
      "Model Saved\n",
      "Episode: 116 Total reward: -72.42770385742188 Training loss: 0.0000 Explore P: 0.6154\n",
      "Episode: 117 Total reward: -48.0035400390625 Training loss: 0.0000 Explore P: 0.6131\n",
      "Episode: 118 Total reward: -105.13510131835938 Training loss: 0.0000 Explore P: 0.6118\n",
      "Model updated\n",
      "Episode: 119 Total reward: -68.1427001953125 Training loss: 0.0000 Explore P: 0.6087\n",
      "Episode: 120 Total reward: -45.996856689453125 Training loss: 0.0000 Explore P: 0.6064\n",
      "Model Saved\n",
      "Episode: 121 Total reward: -7.08074951171875 Training loss: 0.0000 Explore P: 0.6041\n",
      "Episode: 122 Total reward: -7.3184814453125 Training loss: 0.0000 Explore P: 0.6018\n",
      "Episode: 123 Total reward: -32.3701171875 Training loss: 0.0000 Explore P: 0.5996\n",
      "Episode: 124 Total reward: -0.73175048828125 Training loss: 0.0000 Explore P: 0.5972\n",
      "Episode: 125 Total reward: -13.302093505859375 Training loss: 0.0000 Explore P: 0.5948\n",
      "Model Saved\n",
      "Episode: 126 Total reward: -61.81849670410156 Training loss: 0.0000 Explore P: 0.5924\n",
      "Episode: 127 Total reward: 27.887069702148438 Training loss: 0.0000 Explore P: 0.5900\n",
      "Episode: 128 Total reward: 46.35560607910156 Training loss: 0.0000 Explore P: 0.5878\n",
      "Episode: 129 Total reward: -65.249267578125 Training loss: 0.0000 Explore P: 0.5864\n",
      "Episode: 130 Total reward: 42.096527099609375 Training loss: 0.0000 Explore P: 0.5841\n",
      "Model Saved\n",
      "Episode: 131 Total reward: -17.809738159179688 Training loss: 0.0000 Explore P: 0.5818\n",
      "Episode: 132 Total reward: -31.565673828125 Training loss: 0.0000 Explore P: 0.5803\n",
      "Episode: 133 Total reward: -48.08570861816406 Training loss: 0.0000 Explore P: 0.5790\n",
      "Episode: 134 Total reward: 20.639373779296875 Training loss: 0.0000 Explore P: 0.5761\n",
      "Episode: 135 Total reward: -49.49449157714844 Training loss: 0.0000 Explore P: 0.5748\n",
      "Model Saved\n",
      "Episode: 136 Total reward: -49.90867614746094 Training loss: 0.0000 Explore P: 0.5726\n",
      "Episode: 137 Total reward: -26.301132202148438 Training loss: 0.0000 Explore P: 0.5707\n",
      "Episode: 138 Total reward: 24.061553955078125 Training loss: 0.0000 Explore P: 0.5684\n",
      "Episode: 139 Total reward: -89.44047546386719 Training loss: 0.0000 Explore P: 0.5679\n",
      "Episode: 140 Total reward: 121.94235229492188 Training loss: 0.0000 Explore P: 0.5647\n",
      "Model Saved\n",
      "Episode: 141 Total reward: -46.17376708984375 Training loss: 0.0000 Explore P: 0.5626\n",
      "Episode: 142 Total reward: -18.510421752929688 Training loss: 0.0000 Explore P: 0.5607\n",
      "Episode: 143 Total reward: 14.563491821289062 Training loss: 0.0000 Explore P: 0.5585\n",
      "Episode: 144 Total reward: -31.65533447265625 Training loss: 0.0000 Explore P: 0.5564\n",
      "Episode: 145 Total reward: -57.912811279296875 Training loss: 0.0000 Explore P: 0.5551\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 40.60443115234375 Training loss: 0.0000 Explore P: 0.5523\n",
      "Episode: 147 Total reward: -50.140289306640625 Training loss: 0.0000 Explore P: 0.5512\n",
      "Episode: 148 Total reward: -31.97552490234375 Training loss: 0.0000 Explore P: 0.5499\n",
      "Episode: 149 Total reward: -51.64372253417969 Training loss: 0.0000 Explore P: 0.5479\n",
      "Episode: 150 Total reward: 24.052597045898438 Training loss: 0.0000 Explore P: 0.5460\n",
      "Model Saved\n",
      "Episode: 151 Total reward: -3.964111328125 Training loss: 0.0000 Explore P: 0.5439\n",
      "Episode: 152 Total reward: -28.935012817382812 Training loss: 0.0000 Explore P: 0.5426\n",
      "Episode: 153 Total reward: 24.622543334960938 Training loss: 0.0000 Explore P: 0.5404\n",
      "Episode: 154 Total reward: -36.741241455078125 Training loss: 0.0000 Explore P: 0.5391\n",
      "Episode: 155 Total reward: 62.74546813964844 Training loss: 0.0000 Explore P: 0.5371\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 27.59716796875 Training loss: 0.0000 Explore P: 0.5352\n",
      "Episode: 157 Total reward: 24.331161499023438 Training loss: 0.0000 Explore P: 0.5331\n",
      "Episode: 158 Total reward: 13.463424682617188 Training loss: 0.0000 Explore P: 0.5310\n",
      "Episode: 159 Total reward: -1.446746826171875 Training loss: 0.0000 Explore P: 0.5290\n",
      "Episode: 160 Total reward: -59.32275390625 Training loss: 0.0000 Explore P: 0.5278\n",
      "Model Saved\n",
      "Episode: 161 Total reward: -67.64511108398438 Training loss: 0.0000 Explore P: 0.5265\n",
      "Episode: 162 Total reward: -65.02348327636719 Training loss: 0.0000 Explore P: 0.5254\n",
      "Episode: 163 Total reward: 46.29725646972656 Training loss: 0.0000 Explore P: 0.5232\n",
      "Episode: 164 Total reward: -42.45741271972656 Training loss: 0.0000 Explore P: 0.5219\n",
      "Episode: 165 Total reward: -4.802734375 Training loss: 0.0000 Explore P: 0.5199\n",
      "Model Saved\n",
      "Episode: 166 Total reward: -36.258392333984375 Training loss: 0.0000 Explore P: 0.5187\n",
      "Episode: 167 Total reward: 2.2730865478515625 Training loss: 0.0000 Explore P: 0.5168\n",
      "Episode: 168 Total reward: 82.87567138671875 Training loss: 0.0000 Explore P: 0.5139\n",
      "Episode: 169 Total reward: 56.627838134765625 Training loss: 0.0000 Explore P: 0.5120\n",
      "Episode: 170 Total reward: 31.825042724609375 Training loss: 0.0000 Explore P: 0.5099\n",
      "Model Saved\n",
      "Episode: 171 Total reward: -65.1441650390625 Training loss: 0.0000 Explore P: 0.5087\n",
      "Episode: 172 Total reward: -46.00048828125 Training loss: 0.0000 Explore P: 0.5076\n",
      "Episode: 173 Total reward: -19.092620849609375 Training loss: 0.0000 Explore P: 0.5063\n",
      "Episode: 174 Total reward: 39.048309326171875 Training loss: 0.0000 Explore P: 0.5038\n",
      "Episode: 175 Total reward: -26.76507568359375 Training loss: 0.0000 Explore P: 0.5020\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 47.55094909667969 Training loss: 0.0000 Explore P: 0.5000\n",
      "Episode: 177 Total reward: 116.94485473632812 Training loss: 0.0000 Explore P: 0.4979\n",
      "Episode: 178 Total reward: -46.56187438964844 Training loss: 0.0000 Explore P: 0.4967\n",
      "Episode: 179 Total reward: 8.228790283203125 Training loss: 0.0000 Explore P: 0.4950\n",
      "Episode: 180 Total reward: -87.20262145996094 Training loss: 0.0000 Explore P: 0.4938\n",
      "Model Saved\n",
      "Episode: 181 Total reward: -50.45252990722656 Training loss: 0.0000 Explore P: 0.4927\n",
      "Episode: 182 Total reward: 4.0660552978515625 Training loss: 0.0000 Explore P: 0.4910\n",
      "Episode: 183 Total reward: -48.112030029296875 Training loss: 0.0000 Explore P: 0.4899\n",
      "Episode: 184 Total reward: 14.549545288085938 Training loss: 0.0000 Explore P: 0.4888\n",
      "Episode: 185 Total reward: 49.90061950683594 Training loss: 0.0000 Explore P: 0.4870\n",
      "Model Saved\n",
      "Episode: 186 Total reward: -52.48396301269531 Training loss: 0.0000 Explore P: 0.4859\n",
      "Episode: 187 Total reward: 51.46687316894531 Training loss: 0.0000 Explore P: 0.4839\n",
      "Episode: 188 Total reward: -18.91436767578125 Training loss: 0.0000 Explore P: 0.4821\n",
      "Episode: 189 Total reward: -46.521575927734375 Training loss: 0.0000 Explore P: 0.4802\n",
      "Episode: 190 Total reward: 66.81486511230469 Training loss: 0.0000 Explore P: 0.4783\n",
      "Model Saved\n",
      "Episode: 191 Total reward: -17.447998046875 Training loss: 0.0000 Explore P: 0.4772\n",
      "Episode: 192 Total reward: 81.98095703125 Training loss: 0.0000 Explore P: 0.4752\n",
      "Episode: 193 Total reward: 98.46188354492188 Training loss: 0.0000 Explore P: 0.4733\n",
      "Episode: 194 Total reward: -63.570037841796875 Training loss: 0.0000 Explore P: 0.4723\n",
      "Episode: 195 Total reward: -2.339447021484375 Training loss: 0.0000 Explore P: 0.4705\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 67.31513977050781 Training loss: 0.0000 Explore P: 0.4686\n",
      "Episode: 197 Total reward: -33.270904541015625 Training loss: 0.0000 Explore P: 0.4674\n",
      "Episode: 198 Total reward: 4.2571868896484375 Training loss: 0.0000 Explore P: 0.4656\n",
      "Episode: 199 Total reward: 57.343902587890625 Training loss: 0.0000 Explore P: 0.4638\n",
      "Episode: 200 Total reward: -1.3200836181640625 Training loss: 0.0000 Explore P: 0.4628\n",
      "Model Saved\n",
      "Episode: 201 Total reward: -1.84381103515625 Training loss: 0.0000 Explore P: 0.4617\n",
      "Episode: 202 Total reward: 14.605087280273438 Training loss: 0.0000 Explore P: 0.4607\n",
      "Episode: 203 Total reward: 12.572769165039062 Training loss: 0.0000 Explore P: 0.4589\n",
      "Episode: 204 Total reward: -41.750579833984375 Training loss: 0.0000 Explore P: 0.4578\n",
      "Episode: 205 Total reward: 80.90805053710938 Training loss: 0.0000 Explore P: 0.4561\n",
      "Model Saved\n",
      "Episode: 206 Total reward: -42.440185546875 Training loss: 0.0000 Explore P: 0.4550\n",
      "Episode: 207 Total reward: -64.26292419433594 Training loss: 0.0000 Explore P: 0.4540\n",
      "Episode: 208 Total reward: 80.251708984375 Training loss: 0.0000 Explore P: 0.4523\n",
      "Episode: 209 Total reward: 149.20083618164062 Training loss: 0.0000 Explore P: 0.4504\n",
      "Episode: 210 Total reward: 65.52825927734375 Training loss: 0.0000 Explore P: 0.4487\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 12.297454833984375 Training loss: 0.0000 Explore P: 0.4471\n",
      "Episode: 212 Total reward: 2.2884979248046875 Training loss: 0.0000 Explore P: 0.4461\n",
      "Episode: 213 Total reward: -51.52166748046875 Training loss: 0.0000 Explore P: 0.4450\n",
      "Episode: 214 Total reward: -30.519363403320312 Training loss: 0.0000 Explore P: 0.4439\n",
      "Episode: 215 Total reward: 91.63963317871094 Training loss: 0.0000 Explore P: 0.4422\n",
      "Model Saved\n",
      "Episode: 216 Total reward: -3.433837890625 Training loss: 0.0000 Explore P: 0.4413\n",
      "Episode: 217 Total reward: 4.0596160888671875 Training loss: 0.0000 Explore P: 0.4396\n",
      "Episode: 218 Total reward: 38.09577941894531 Training loss: 0.0000 Explore P: 0.4377\n",
      "Episode: 219 Total reward: -33.20945739746094 Training loss: 0.0000 Explore P: 0.4366\n",
      "Episode: 220 Total reward: 63.090576171875 Training loss: 0.0000 Explore P: 0.4348\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 33.0062255859375 Training loss: 0.0000 Explore P: 0.4337\n",
      "Episode: 222 Total reward: -72.39665222167969 Training loss: 0.0000 Explore P: 0.4327\n",
      "Episode: 223 Total reward: -32.59040832519531 Training loss: 0.0000 Explore P: 0.4311\n",
      "Episode: 224 Total reward: -13.040939331054688 Training loss: 0.0000 Explore P: 0.4296\n",
      "Episode: 225 Total reward: 71.65997314453125 Training loss: 0.0000 Explore P: 0.4279\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 109.26040649414062 Training loss: 0.0000 Explore P: 0.4262\n",
      "Episode: 227 Total reward: -16.43231201171875 Training loss: 0.0000 Explore P: 0.4247\n",
      "Episode: 228 Total reward: -45.32240295410156 Training loss: 0.0000 Explore P: 0.4236\n",
      "Episode: 229 Total reward: 67.68881225585938 Training loss: 0.0000 Explore P: 0.4220\n",
      "Episode: 230 Total reward: 52.22019958496094 Training loss: 0.0000 Explore P: 0.4205\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 64.32868957519531 Training loss: 0.0000 Explore P: 0.4188\n",
      "Episode: 232 Total reward: -62.97148132324219 Training loss: 0.0000 Explore P: 0.4178\n",
      "Episode: 233 Total reward: -25.264739990234375 Training loss: 0.0000 Explore P: 0.4163\n",
      "Episode: 234 Total reward: -32.32817077636719 Training loss: 0.0000 Explore P: 0.4154\n",
      "Episode: 235 Total reward: -44.32945251464844 Training loss: 0.0000 Explore P: 0.4138\n",
      "Model Saved\n",
      "Episode: 236 Total reward: -21.457107543945312 Training loss: 0.0000 Explore P: 0.4129\n",
      "Episode: 237 Total reward: 28.731674194335938 Training loss: 0.0000 Explore P: 0.4113\n",
      "Episode: 238 Total reward: 104.2388916015625 Training loss: 0.0000 Explore P: 0.4096\n",
      "Episode: 239 Total reward: -63.23265075683594 Training loss: 0.0000 Explore P: 0.4086\n",
      "Episode: 240 Total reward: 56.991790771484375 Training loss: 0.0000 Explore P: 0.4070\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 75.66926574707031 Training loss: 0.0000 Explore P: 0.4055\n",
      "Episode: 242 Total reward: -50.23095703125 Training loss: 0.0000 Explore P: 0.4045\n",
      "Episode: 243 Total reward: 8.358413696289062 Training loss: 0.0000 Explore P: 0.4036\n",
      "Episode: 244 Total reward: -12.518814086914062 Training loss: 0.0000 Explore P: 0.4026\n",
      "Episode: 245 Total reward: -94.32330322265625 Training loss: 0.0000 Explore P: 0.4016\n",
      "Model Saved\n",
      "Episode: 246 Total reward: -46.34226989746094 Training loss: 0.0000 Explore P: 0.4007\n",
      "Episode: 247 Total reward: 74.54583740234375 Training loss: 0.0000 Explore P: 0.3992\n",
      "Episode: 248 Total reward: 98.04960632324219 Training loss: 0.0000 Explore P: 0.3977\n",
      "Episode: 249 Total reward: 27.5670166015625 Training loss: 0.0000 Explore P: 0.3961\n",
      "Episode: 250 Total reward: -2.7335968017578125 Training loss: 0.0000 Explore P: 0.3952\n",
      "Model Saved\n",
      "Episode: 251 Total reward: -51.66419982910156 Training loss: 0.0000 Explore P: 0.3943\n",
      "Episode: 252 Total reward: 31.582794189453125 Training loss: 0.0000 Explore P: 0.3928\n",
      "Episode: 253 Total reward: 53.10589599609375 Training loss: 0.0000 Explore P: 0.3912\n",
      "Episode: 254 Total reward: -29.291183471679688 Training loss: 0.0000 Explore P: 0.3902\n",
      "Episode: 255 Total reward: 83.65791320800781 Training loss: 0.0000 Explore P: 0.3888\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 66.65855407714844 Training loss: 0.0000 Explore P: 0.3874\n",
      "Episode: 257 Total reward: 113.197021484375 Training loss: 0.0000 Explore P: 0.3859\n",
      "Episode: 258 Total reward: -21.65985107421875 Training loss: 0.0000 Explore P: 0.3846\n",
      "Episode: 259 Total reward: 167.32139587402344 Training loss: 0.0000 Explore P: 0.3826\n",
      "Episode: 260 Total reward: 4.42803955078125 Training loss: 0.0000 Explore P: 0.3816\n",
      "Model Saved\n",
      "Episode: 261 Total reward: -46.01092529296875 Training loss: 0.0000 Explore P: 0.3806\n",
      "Episode: 262 Total reward: 42.33384704589844 Training loss: 0.0000 Explore P: 0.3797\n",
      "Episode: 263 Total reward: 121.0966796875 Training loss: 0.0000 Explore P: 0.3782\n",
      "Episode: 264 Total reward: 50.09812927246094 Training loss: 0.0000 Explore P: 0.3769\n",
      "Episode: 265 Total reward: -29.255325317382812 Training loss: 0.0000 Explore P: 0.3760\n",
      "Model Saved\n",
      "Episode: 266 Total reward: -15.89208984375 Training loss: 0.0000 Explore P: 0.3752\n",
      "Episode: 267 Total reward: -23.609146118164062 Training loss: 0.0000 Explore P: 0.3743\n",
      "Model updated\n",
      "Episode: 268 Total reward: -15.676910400390625 Training loss: 0.0000 Explore P: 0.3735\n",
      "Episode: 269 Total reward: 72.10618591308594 Training loss: 0.0000 Explore P: 0.3721\n",
      "Episode: 270 Total reward: 0.89605712890625 Training loss: 0.0000 Explore P: 0.3712\n",
      "Model Saved\n",
      "Episode: 271 Total reward: -45.96742248535156 Training loss: 0.0000 Explore P: 0.3703\n",
      "Episode: 272 Total reward: 13.968017578125 Training loss: 0.0000 Explore P: 0.3689\n",
      "Episode: 273 Total reward: -39.60960388183594 Training loss: 0.0000 Explore P: 0.3680\n",
      "Episode: 274 Total reward: 37.45823669433594 Training loss: 0.0000 Explore P: 0.3667\n",
      "Episode: 275 Total reward: 56.01708984375 Training loss: 0.0000 Explore P: 0.3655\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 2.477783203125 Training loss: 0.0000 Explore P: 0.3647\n",
      "Episode: 277 Total reward: 45.52442932128906 Training loss: 0.0000 Explore P: 0.3633\n",
      "Episode: 278 Total reward: 128.3403778076172 Training loss: 0.0000 Explore P: 0.3620\n",
      "Episode: 279 Total reward: 41.57012939453125 Training loss: 0.0000 Explore P: 0.3605\n",
      "Episode: 280 Total reward: -12.036163330078125 Training loss: 0.0000 Explore P: 0.3596\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 105.15567016601562 Training loss: 0.0000 Explore P: 0.3583\n",
      "Episode: 282 Total reward: 55.3302001953125 Training loss: 0.0000 Explore P: 0.3570\n",
      "Episode: 283 Total reward: 58.803253173828125 Training loss: 0.0000 Explore P: 0.3557\n",
      "Episode: 284 Total reward: 109.02456665039062 Training loss: 0.0000 Explore P: 0.3545\n",
      "Episode: 285 Total reward: 243.1256561279297 Training loss: 0.0000 Explore P: 0.3525\n",
      "Model Saved\n",
      "Episode: 286 Total reward: -28.253616333007812 Training loss: 0.0000 Explore P: 0.3517\n",
      "Episode: 287 Total reward: -22.1861572265625 Training loss: 0.0000 Explore P: 0.3508\n",
      "Episode: 288 Total reward: 130.233642578125 Training loss: 0.0000 Explore P: 0.3496\n",
      "Episode: 289 Total reward: 128.55223083496094 Training loss: 0.0000 Explore P: 0.3482\n",
      "Episode: 290 Total reward: 71.94271850585938 Training loss: 0.0000 Explore P: 0.3470\n",
      "Model Saved\n",
      "Episode: 291 Total reward: -14.783721923828125 Training loss: 0.0000 Explore P: 0.3462\n",
      "Episode: 292 Total reward: -49.48197937011719 Training loss: 0.0000 Explore P: 0.3455\n",
      "Episode: 293 Total reward: 82.6971435546875 Training loss: 0.0000 Explore P: 0.3446\n",
      "Episode: 294 Total reward: 130.8936309814453 Training loss: 0.0000 Explore P: 0.3432\n",
      "Episode: 295 Total reward: 57.49568176269531 Training loss: 0.0000 Explore P: 0.3419\n",
      "Model Saved\n",
      "Episode: 296 Total reward: -62.399566650390625 Training loss: 0.0000 Explore P: 0.3412\n",
      "Episode: 297 Total reward: -8.547698974609375 Training loss: 0.0000 Explore P: 0.3400\n",
      "Episode: 298 Total reward: 13.743057250976562 Training loss: 0.0000 Explore P: 0.3388\n",
      "Episode: 299 Total reward: -70.04219055175781 Training loss: 0.0000 Explore P: 0.3380\n",
      "Episode: 300 Total reward: 46.379913330078125 Training loss: 0.0000 Explore P: 0.3367\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 15.927749633789062 Training loss: 0.0000 Explore P: 0.3354\n",
      "Episode: 302 Total reward: 184.30450439453125 Training loss: 0.0000 Explore P: 0.3342\n",
      "Episode: 303 Total reward: 38.42536926269531 Training loss: 0.0000 Explore P: 0.3330\n",
      "Episode: 304 Total reward: 11.730941772460938 Training loss: 0.0000 Explore P: 0.3322\n",
      "Episode: 305 Total reward: -23.592926025390625 Training loss: 0.0000 Explore P: 0.3314\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 0.508575439453125 Training loss: 0.0000 Explore P: 0.3306\n",
      "Episode: 307 Total reward: 30.316558837890625 Training loss: 0.0000 Explore P: 0.3294\n",
      "Episode: 308 Total reward: 302.7485046386719 Training loss: 0.0000 Explore P: 0.3283\n",
      "Episode: 309 Total reward: 67.99331665039062 Training loss: 0.0000 Explore P: 0.3270\n",
      "Episode: 310 Total reward: -13.964752197265625 Training loss: 0.0000 Explore P: 0.3259\n",
      "Model Saved\n",
      "Episode: 311 Total reward: -82.83602905273438 Training loss: 0.0000 Explore P: 0.3251\n",
      "Episode: 312 Total reward: -5.814727783203125 Training loss: 0.0000 Explore P: 0.3243\n",
      "Episode: 313 Total reward: 57.01177978515625 Training loss: 0.0000 Explore P: 0.3231\n",
      "Episode: 314 Total reward: 35.46311950683594 Training loss: 0.0000 Explore P: 0.3224\n",
      "Episode: 315 Total reward: -9.68817138671875 Training loss: 0.0000 Explore P: 0.3216\n",
      "Model Saved\n",
      "Episode: 316 Total reward: -25.9747314453125 Training loss: 0.0000 Explore P: 0.3209\n",
      "Episode: 317 Total reward: 297.61183166503906 Training loss: 0.0000 Explore P: 0.3195\n",
      "Episode: 318 Total reward: 40.733489990234375 Training loss: 0.0000 Explore P: 0.3187\n",
      "Episode: 319 Total reward: 133.55758666992188 Training loss: 0.0000 Explore P: 0.3174\n",
      "Episode: 320 Total reward: 124.52079772949219 Training loss: 0.0000 Explore P: 0.3163\n",
      "Model Saved\n",
      "Episode: 321 Total reward: -68.99502563476562 Training loss: 0.0000 Explore P: 0.3155\n",
      "Episode: 322 Total reward: 364.3259582519531 Training loss: 0.0000 Explore P: 0.3134\n",
      "Episode: 323 Total reward: 54.525146484375 Training loss: 0.0000 Explore P: 0.3121\n",
      "Episode: 324 Total reward: 72.07858276367188 Training loss: 0.0000 Explore P: 0.3110\n",
      "Episode: 325 Total reward: -57.706817626953125 Training loss: 0.0000 Explore P: 0.3102\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 14.267562866210938 Training loss: 0.0000 Explore P: 0.3094\n",
      "Episode: 327 Total reward: 126.51739501953125 Training loss: 0.0000 Explore P: 0.3083\n",
      "Episode: 328 Total reward: 55.3558349609375 Training loss: 0.0000 Explore P: 0.3071\n",
      "Episode: 329 Total reward: 7.3203277587890625 Training loss: 0.0000 Explore P: 0.3064\n",
      "Episode: 330 Total reward: 219.18495178222656 Training loss: 0.0000 Explore P: 0.3047\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 20.967269897460938 Training loss: 0.0000 Explore P: 0.3040\n",
      "Episode: 332 Total reward: -38.891357421875 Training loss: 0.0000 Explore P: 0.3032\n",
      "Episode: 333 Total reward: 113.49978637695312 Training loss: 0.0000 Explore P: 0.3021\n",
      "Episode: 334 Total reward: 97.96272277832031 Training loss: 0.0000 Explore P: 0.3009\n",
      "Episode: 335 Total reward: 217.31216430664062 Training loss: 0.0000 Explore P: 0.2997\n",
      "Model Saved\n",
      "Episode: 336 Total reward: -37.49113464355469 Training loss: 0.0000 Explore P: 0.2991\n",
      "Episode: 337 Total reward: -37.514129638671875 Training loss: 0.0000 Explore P: 0.2983\n",
      "Episode: 338 Total reward: 150.52880859375 Training loss: 0.0000 Explore P: 0.2972\n",
      "Episode: 339 Total reward: 404.9656524658203 Training loss: 0.0000 Explore P: 0.2956\n",
      "Episode: 340 Total reward: -38.53846740722656 Training loss: 0.0000 Explore P: 0.2950\n",
      "Model Saved\n",
      "Episode: 341 Total reward: -50.96208190917969 Training loss: 0.0000 Explore P: 0.2943\n",
      "Episode: 342 Total reward: 24.053634643554688 Training loss: 0.0000 Explore P: 0.2936\n",
      "Episode: 343 Total reward: 302.6991729736328 Training loss: 0.0000 Explore P: 0.2920\n",
      "Episode: 344 Total reward: 116.70352172851562 Training loss: 0.0000 Explore P: 0.2908\n",
      "Episode: 345 Total reward: 101.0731201171875 Training loss: 0.0000 Explore P: 0.2896\n",
      "Model Saved\n",
      "Episode: 346 Total reward: -37.59129333496094 Training loss: 0.0000 Explore P: 0.2890\n",
      "Episode: 347 Total reward: 162.37449645996094 Training loss: 0.0000 Explore P: 0.2878\n",
      "Episode: 348 Total reward: -32.225494384765625 Training loss: 0.0000 Explore P: 0.2872\n",
      "Episode: 349 Total reward: 655.8532104492188 Training loss: 0.0000 Explore P: 0.2850\n",
      "Episode: 350 Total reward: 72.07546997070312 Training loss: 0.0000 Explore P: 0.2840\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 187.10597229003906 Training loss: 0.0000 Explore P: 0.2829\n",
      "Episode: 352 Total reward: 371.31317138671875 Training loss: 0.0000 Explore P: 0.2813\n",
      "Episode: 353 Total reward: 67.78276062011719 Training loss: 0.0000 Explore P: 0.2803\n",
      "Episode: 354 Total reward: 79.53038024902344 Training loss: 0.0000 Explore P: 0.2791\n",
      "Episode: 355 Total reward: -38.564361572265625 Training loss: 0.0000 Explore P: 0.2784\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 119.68942260742188 Training loss: 0.0000 Explore P: 0.2774\n",
      "Episode: 357 Total reward: 231.33270263671875 Training loss: 0.0000 Explore P: 0.2762\n",
      "Episode: 358 Total reward: 132.5276336669922 Training loss: 0.0000 Explore P: 0.2752\n",
      "Episode: 359 Total reward: 131.0966339111328 Training loss: 0.0000 Explore P: 0.2741\n",
      "Episode: 360 Total reward: -60.13920593261719 Training loss: 0.0000 Explore P: 0.2735\n",
      "Model Saved\n",
      "Episode: 361 Total reward: -16.063980102539062 Training loss: 0.0000 Explore P: 0.2729\n",
      "Episode: 362 Total reward: 87.98866271972656 Training loss: 0.0000 Explore P: 0.2719\n",
      "Episode: 363 Total reward: 116.96908569335938 Training loss: 0.0000 Explore P: 0.2709\n",
      "Episode: 364 Total reward: 4.0822906494140625 Training loss: 0.0000 Explore P: 0.2703\n",
      "Episode: 365 Total reward: 100.26763916015625 Training loss: 0.0000 Explore P: 0.2693\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 65.38858032226562 Training loss: 0.0000 Explore P: 0.2683\n",
      "Episode: 367 Total reward: 75.36677551269531 Training loss: 0.0000 Explore P: 0.2673\n",
      "Episode: 368 Total reward: 128.2577667236328 Training loss: 0.0000 Explore P: 0.2663\n",
      "Episode: 369 Total reward: 11.558395385742188 Training loss: 0.0000 Explore P: 0.2656\n",
      "Episode: 370 Total reward: 105.52668762207031 Training loss: 0.0000 Explore P: 0.2650\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 34.32841491699219 Training loss: 0.0000 Explore P: 0.2643\n",
      "Episode: 372 Total reward: 96.3543701171875 Training loss: 0.0000 Explore P: 0.2633\n",
      "Episode: 373 Total reward: 7.47314453125 Training loss: 0.0000 Explore P: 0.2628\n",
      "Episode: 374 Total reward: -25.047836303710938 Training loss: 0.0000 Explore P: 0.2621\n",
      "Episode: 375 Total reward: 219.46652221679688 Training loss: 0.0000 Explore P: 0.2611\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 149.5258026123047 Training loss: 0.0000 Explore P: 0.2602\n",
      "Episode: 377 Total reward: 147.53155517578125 Training loss: 0.0000 Explore P: 0.2592\n",
      "Episode: 378 Total reward: -15.894119262695312 Training loss: 0.0000 Explore P: 0.2586\n",
      "Episode: 379 Total reward: 51.32933044433594 Training loss: 0.0000 Explore P: 0.2579\n",
      "Episode: 380 Total reward: 237.043212890625 Training loss: 0.0000 Explore P: 0.2568\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 159.95101928710938 Training loss: 0.0000 Explore P: 0.2555\n",
      "Episode: 382 Total reward: 82.16043090820312 Training loss: 0.0000 Explore P: 0.2545\n",
      "Episode: 383 Total reward: -38.04014587402344 Training loss: 0.0000 Explore P: 0.2539\n",
      "Episode: 384 Total reward: 23.50433349609375 Training loss: 0.0000 Explore P: 0.2533\n",
      "Episode: 385 Total reward: 68.11695861816406 Training loss: 0.0000 Explore P: 0.2523\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 183.89739990234375 Training loss: 0.0000 Explore P: 0.2513\n",
      "Episode: 387 Total reward: 47.08551025390625 Training loss: 0.0000 Explore P: 0.2507\n",
      "Episode: 388 Total reward: 64.6763916015625 Training loss: 0.0000 Explore P: 0.2498\n",
      "Episode: 389 Total reward: -0.17279052734375 Training loss: 0.0000 Explore P: 0.2492\n",
      "Episode: 390 Total reward: -17.353713989257812 Training loss: 0.0000 Explore P: 0.2486\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 71.31829833984375 Training loss: 0.0000 Explore P: 0.2480\n",
      "Episode: 392 Total reward: 42.32945251464844 Training loss: 0.0000 Explore P: 0.2472\n",
      "Episode: 393 Total reward: 110.53773498535156 Training loss: 0.0000 Explore P: 0.2462\n",
      "Episode: 394 Total reward: 4.3724212646484375 Training loss: 0.0000 Explore P: 0.2457\n",
      "Episode: 395 Total reward: 46.335845947265625 Training loss: 0.0000 Explore P: 0.2448\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 127.8624267578125 Training loss: 0.0000 Explore P: 0.2439\n",
      "Episode: 397 Total reward: -39.83360290527344 Training loss: 0.0000 Explore P: 0.2434\n",
      "Episode: 398 Total reward: 215.76571655273438 Training loss: 0.0000 Explore P: 0.2425\n",
      "Episode: 399 Total reward: 52.52197265625 Training loss: 0.0000 Explore P: 0.2420\n",
      "Episode: 400 Total reward: -61.253753662109375 Training loss: 0.0000 Explore P: 0.2414\n",
      "Model Saved\n",
      "Episode: 401 Total reward: -23.34393310546875 Training loss: 0.0000 Explore P: 0.2409\n",
      "Episode: 402 Total reward: -1.085601806640625 Training loss: 0.0000 Explore P: 0.2403\n",
      "Episode: 403 Total reward: 602.4681854248047 Training loss: 0.0000 Explore P: 0.2382\n",
      "Episode: 404 Total reward: 358.22935485839844 Training loss: 0.0000 Explore P: 0.2373\n",
      "Episode: 405 Total reward: 220.79962158203125 Training loss: 0.0000 Explore P: 0.2363\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 142.91815185546875 Training loss: 0.0000 Explore P: 0.2354\n",
      "Episode: 407 Total reward: 8.223190307617188 Training loss: 0.0000 Explore P: 0.2349\n",
      "Episode: 408 Total reward: -2.7475128173828125 Training loss: 0.0000 Explore P: 0.2343\n",
      "Episode: 409 Total reward: 176.4046173095703 Training loss: 0.0000 Explore P: 0.2334\n",
      "Episode: 410 Total reward: 32.0396728515625 Training loss: 0.0000 Explore P: 0.2329\n",
      "Model Saved\n",
      "Episode: 411 Total reward: -1.1584014892578125 Training loss: 0.0000 Explore P: 0.2324\n",
      "Episode: 412 Total reward: -20.177337646484375 Training loss: 0.0000 Explore P: 0.2318\n",
      "Episode: 413 Total reward: 80.24546813964844 Training loss: 0.0000 Explore P: 0.2310\n",
      "Model updated\n",
      "Episode: 414 Total reward: 42.043060302734375 Training loss: 0.0000 Explore P: 0.2305\n",
      "Episode: 415 Total reward: 97.01841735839844 Training loss: 0.0000 Explore P: 0.2296\n",
      "Model Saved\n",
      "Episode: 416 Total reward: -48.102630615234375 Training loss: 0.0000 Explore P: 0.2291\n",
      "Episode: 417 Total reward: 37.041107177734375 Training loss: 0.0000 Explore P: 0.2286\n",
      "Episode: 418 Total reward: 29.420501708984375 Training loss: 0.0000 Explore P: 0.2280\n",
      "Episode: 419 Total reward: -3.7545928955078125 Training loss: 0.0000 Explore P: 0.2275\n",
      "Episode: 420 Total reward: 124.28016662597656 Training loss: 0.0000 Explore P: 0.2267\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 210.04022216796875 Training loss: 0.0000 Explore P: 0.2258\n",
      "Episode: 422 Total reward: 1.8267974853515625 Training loss: 0.0000 Explore P: 0.2253\n",
      "Episode: 423 Total reward: -10.81854248046875 Training loss: 0.0000 Explore P: 0.2248\n",
      "Episode: 424 Total reward: 79.20185852050781 Training loss: 0.0000 Explore P: 0.2240\n",
      "Episode: 425 Total reward: 91.45472717285156 Training loss: 0.0000 Explore P: 0.2234\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 7.4172515869140625 Training loss: 0.0000 Explore P: 0.2229\n",
      "Episode: 427 Total reward: 90.09878540039062 Training loss: 0.0000 Explore P: 0.2221\n",
      "Episode: 428 Total reward: 153.47755432128906 Training loss: 0.0000 Explore P: 0.2212\n",
      "Episode: 429 Total reward: -28.096710205078125 Training loss: 0.0000 Explore P: 0.2207\n",
      "Episode: 430 Total reward: -29.042221069335938 Training loss: 0.0000 Explore P: 0.2202\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 5.15460205078125 Training loss: 0.0000 Explore P: 0.2197\n",
      "Episode: 432 Total reward: 151.13226318359375 Training loss: 0.0000 Explore P: 0.2189\n",
      "Episode: 433 Total reward: 77.03456115722656 Training loss: 0.0000 Explore P: 0.2181\n",
      "Episode: 434 Total reward: 160.6336212158203 Training loss: 0.0000 Explore P: 0.2173\n",
      "Episode: 435 Total reward: 409.2026824951172 Training loss: 0.0000 Explore P: 0.2163\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 178.8981475830078 Training loss: 0.0000 Explore P: 0.2155\n",
      "Episode: 437 Total reward: 24.675018310546875 Training loss: 0.0000 Explore P: 0.2150\n",
      "Episode: 438 Total reward: 4.4635009765625 Training loss: 0.0000 Explore P: 0.2145\n",
      "Episode: 439 Total reward: -104.62770080566406 Training loss: 0.0000 Explore P: 0.2140\n",
      "Episode: 440 Total reward: -28.903228759765625 Training loss: 0.0000 Explore P: 0.2135\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 47.38203430175781 Training loss: 0.0000 Explore P: 0.2130\n",
      "Episode: 442 Total reward: -43.168975830078125 Training loss: 0.0000 Explore P: 0.2125\n",
      "Episode: 443 Total reward: 74.73696899414062 Training loss: 0.0000 Explore P: 0.2118\n",
      "Episode: 444 Total reward: 42.24723815917969 Training loss: 0.0000 Explore P: 0.2111\n",
      "Episode: 445 Total reward: 83.50651550292969 Training loss: 0.0000 Explore P: 0.2104\n",
      "Model Saved\n",
      "Episode: 446 Total reward: -15.742263793945312 Training loss: 0.0000 Explore P: 0.2100\n",
      "Episode: 447 Total reward: 226.4458465576172 Training loss: 0.0000 Explore P: 0.2092\n",
      "Episode: 448 Total reward: 60.88836669921875 Training loss: 0.0000 Explore P: 0.2084\n",
      "Episode: 449 Total reward: -23.142242431640625 Training loss: 0.0000 Explore P: 0.2079\n",
      "Episode: 450 Total reward: 126.17578125 Training loss: 0.0000 Explore P: 0.2072\n",
      "Model Saved\n",
      "Episode: 451 Total reward: -25.75933837890625 Training loss: 0.0000 Explore P: 0.2067\n",
      "Episode: 452 Total reward: 178.61607360839844 Training loss: 0.0000 Explore P: 0.2056\n",
      "Episode: 453 Total reward: -38.89537048339844 Training loss: 0.0000 Explore P: 0.2051\n",
      "Episode: 454 Total reward: -7.2791290283203125 Training loss: 0.0000 Explore P: 0.2046\n",
      "Episode: 455 Total reward: 199.98513793945312 Training loss: 0.0000 Explore P: 0.2039\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 156.00917053222656 Training loss: 0.0000 Explore P: 0.2031\n",
      "Episode: 457 Total reward: 118.97221374511719 Training loss: 0.0000 Explore P: 0.2024\n",
      "Episode: 458 Total reward: 199.64688110351562 Training loss: 0.0000 Explore P: 0.2016\n",
      "Episode: 459 Total reward: 114.97502136230469 Training loss: 0.0000 Explore P: 0.2009\n",
      "Episode: 460 Total reward: 165.0277557373047 Training loss: 0.0000 Explore P: 0.2001\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 240.8494110107422 Training loss: 0.0000 Explore P: 0.1994\n",
      "Episode: 462 Total reward: -12.777786254882812 Training loss: 0.0000 Explore P: 0.1990\n",
      "Episode: 463 Total reward: 87.09426879882812 Training loss: 0.0000 Explore P: 0.1982\n",
      "Episode: 464 Total reward: 50.17610168457031 Training loss: 0.0000 Explore P: 0.1975\n",
      "Episode: 465 Total reward: 112.60708618164062 Training loss: 0.0000 Explore P: 0.1968\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 46.40899658203125 Training loss: 0.0000 Explore P: 0.1961\n",
      "Episode: 467 Total reward: 20.584716796875 Training loss: 0.0000 Explore P: 0.1956\n",
      "Episode: 468 Total reward: 94.63655090332031 Training loss: 0.0000 Explore P: 0.1950\n",
      "Episode: 469 Total reward: -41.27989196777344 Training loss: 0.0000 Explore P: 0.1945\n",
      "Episode: 470 Total reward: 141.790771484375 Training loss: 0.0000 Explore P: 0.1937\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 32.59355163574219 Training loss: 0.0000 Explore P: 0.1933\n",
      "Episode: 472 Total reward: -53.74723815917969 Training loss: 0.0000 Explore P: 0.1929\n",
      "Episode: 473 Total reward: 19.757217407226562 Training loss: 0.0000 Explore P: 0.1924\n",
      "Episode: 474 Total reward: 34.68540954589844 Training loss: 0.0000 Explore P: 0.1920\n",
      "Episode: 475 Total reward: 5.4823455810546875 Training loss: 0.0000 Explore P: 0.1916\n",
      "Model Saved\n",
      "Episode: 476 Total reward: -3.0317535400390625 Training loss: 0.0000 Explore P: 0.1912\n",
      "Episode: 477 Total reward: 101.65548706054688 Training loss: 0.0000 Explore P: 0.1905\n",
      "Episode: 478 Total reward: 45.17889404296875 Training loss: 0.0000 Explore P: 0.1900\n",
      "Episode: 479 Total reward: 57.569305419921875 Training loss: 0.0000 Explore P: 0.1894\n",
      "Episode: 480 Total reward: 344.96092224121094 Training loss: 0.0000 Explore P: 0.1884\n",
      "Model Saved\n",
      "Episode: 481 Total reward: -0.2272796630859375 Training loss: 0.0000 Explore P: 0.1880\n",
      "Episode: 482 Total reward: 100.66751098632812 Training loss: 0.0000 Explore P: 0.1875\n",
      "Episode: 483 Total reward: -32.17088317871094 Training loss: 0.0000 Explore P: 0.1871\n",
      "Episode: 484 Total reward: 214.45947265625 Training loss: 0.0000 Explore P: 0.1865\n",
      "Episode: 485 Total reward: 331.9988708496094 Training loss: 0.0000 Explore P: 0.1857\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 215.6240692138672 Training loss: 0.0000 Explore P: 0.1850\n",
      "Episode: 487 Total reward: -43.88829040527344 Training loss: 0.0000 Explore P: 0.1846\n",
      "Episode: 488 Total reward: 26.10504150390625 Training loss: 0.0000 Explore P: 0.1842\n",
      "Episode: 489 Total reward: 79.23368835449219 Training loss: 0.0000 Explore P: 0.1835\n",
      "Episode: 490 Total reward: -23.345001220703125 Training loss: 0.0000 Explore P: 0.1830\n",
      "Model Saved\n",
      "Episode: 491 Total reward: -50.697113037109375 Training loss: 0.0000 Explore P: 0.1826\n",
      "Episode: 492 Total reward: 72.37503051757812 Training loss: 0.0000 Explore P: 0.1820\n",
      "Episode: 493 Total reward: 244.5807342529297 Training loss: 0.0000 Explore P: 0.1813\n",
      "Episode: 494 Total reward: 73.19889831542969 Training loss: 0.0000 Explore P: 0.1809\n",
      "Episode: 495 Total reward: 38.43925476074219 Training loss: 0.0000 Explore P: 0.1804\n",
      "Model Saved\n",
      "Episode: 496 Total reward: -70.00535583496094 Training loss: 0.0000 Explore P: 0.1803\n",
      "Episode: 497 Total reward: 20.264450073242188 Training loss: 0.0000 Explore P: 0.1797\n",
      "Episode: 498 Total reward: 35.13671875 Training loss: 0.0000 Explore P: 0.1793\n",
      "Episode: 499 Total reward: 19.908050537109375 Training loss: 0.0000 Explore P: 0.1789\n",
      "Episode: 500 Total reward: 113.40345764160156 Training loss: 0.0000 Explore P: 0.1780\n",
      "Model Saved\n",
      "Episode: 501 Total reward: -7.826507568359375 Training loss: 0.0000 Explore P: 0.1776\n",
      "Episode: 502 Total reward: 123.19285583496094 Training loss: 0.0000 Explore P: 0.1770\n",
      "Episode: 503 Total reward: 235.82766723632812 Training loss: 0.0000 Explore P: 0.1763\n",
      "Episode: 504 Total reward: 37.271820068359375 Training loss: 0.0000 Explore P: 0.1758\n",
      "Episode: 505 Total reward: 186.75906372070312 Training loss: 0.0000 Explore P: 0.1752\n",
      "Model Saved\n",
      "Episode: 506 Total reward: -51.64300537109375 Training loss: 0.0000 Explore P: 0.1748\n",
      "Episode: 507 Total reward: 1.4195404052734375 Training loss: 0.0000 Explore P: 0.1744\n",
      "Episode: 508 Total reward: 56.38267517089844 Training loss: 0.0000 Explore P: 0.1741\n",
      "Episode: 509 Total reward: -39.11668395996094 Training loss: 0.0000 Explore P: 0.1737\n",
      "Episode: 510 Total reward: 275.4307861328125 Training loss: 0.0000 Explore P: 0.1729\n",
      "Model Saved\n",
      "Episode: 511 Total reward: 44.9678955078125 Training loss: 0.0000 Explore P: 0.1723\n",
      "Episode: 512 Total reward: 45.018585205078125 Training loss: 0.0000 Explore P: 0.1719\n",
      "Episode: 513 Total reward: 372.6607666015625 Training loss: 0.0000 Explore P: 0.1711\n",
      "Episode: 514 Total reward: -35.08503723144531 Training loss: 0.0000 Explore P: 0.1708\n",
      "Episode: 515 Total reward: 2.17376708984375 Training loss: 0.0000 Explore P: 0.1704\n",
      "Model Saved\n",
      "Episode: 516 Total reward: 47.10014343261719 Training loss: 0.0000 Explore P: 0.1700\n",
      "Episode: 517 Total reward: -39.17646789550781 Training loss: 0.0000 Explore P: 0.1696\n",
      "Episode: 518 Total reward: 71.98385620117188 Training loss: 0.0000 Explore P: 0.1690\n",
      "Episode: 519 Total reward: 134.47055053710938 Training loss: 0.0000 Explore P: 0.1684\n",
      "Episode: 520 Total reward: -34.567779541015625 Training loss: 0.0000 Explore P: 0.1680\n",
      "Model Saved\n",
      "Episode: 521 Total reward: -6.856658935546875 Training loss: 0.0000 Explore P: 0.1677\n",
      "Episode: 522 Total reward: 519.0879364013672 Training loss: 0.0000 Explore P: 0.1667\n",
      "Episode: 523 Total reward: -34.322906494140625 Training loss: 0.0000 Explore P: 0.1664\n",
      "Episode: 524 Total reward: 6.639007568359375 Training loss: 0.0000 Explore P: 0.1660\n",
      "Episode: 525 Total reward: 178.17013549804688 Training loss: 0.0000 Explore P: 0.1654\n",
      "Model Saved\n",
      "Episode: 526 Total reward: 118.77738952636719 Training loss: 0.0000 Explore P: 0.1648\n",
      "Episode: 527 Total reward: 87.10888671875 Training loss: 0.0000 Explore P: 0.1642\n",
      "Episode: 528 Total reward: -13.56414794921875 Training loss: 0.0000 Explore P: 0.1638\n",
      "Episode: 529 Total reward: 22.276947021484375 Training loss: 0.0000 Explore P: 0.1634\n",
      "Episode: 530 Total reward: -100.18284606933594 Training loss: 0.0000 Explore P: 0.1633\n",
      "Model Saved\n",
      "Episode: 531 Total reward: 392.2187042236328 Training loss: 0.0000 Explore P: 0.1626\n",
      "Episode: 532 Total reward: 75.79733276367188 Training loss: 0.0000 Explore P: 0.1622\n",
      "Episode: 533 Total reward: -50.38600158691406 Training loss: 0.0000 Explore P: 0.1618\n",
      "Episode: 534 Total reward: 386.6471710205078 Training loss: 0.0000 Explore P: 0.1610\n",
      "Episode: 535 Total reward: 63.51167297363281 Training loss: 0.0000 Explore P: 0.1605\n",
      "Model Saved\n",
      "Episode: 536 Total reward: 416.2896728515625 Training loss: 0.0000 Explore P: 0.1596\n",
      "Episode: 537 Total reward: 52.079193115234375 Training loss: 0.0000 Explore P: 0.1591\n",
      "Episode: 538 Total reward: -11.870132446289062 Training loss: 0.0000 Explore P: 0.1587\n",
      "Episode: 539 Total reward: 211.6298065185547 Training loss: 0.0000 Explore P: 0.1581\n",
      "Episode: 540 Total reward: 214.36561584472656 Training loss: 0.0000 Explore P: 0.1576\n",
      "Model Saved\n",
      "Episode: 541 Total reward: 158.43075561523438 Training loss: 0.0000 Explore P: 0.1570\n",
      "Episode: 542 Total reward: -21.312896728515625 Training loss: 0.0000 Explore P: 0.1567\n",
      "Episode: 543 Total reward: 105.75093078613281 Training loss: 0.0000 Explore P: 0.1561\n",
      "Episode: 544 Total reward: 2.4994049072265625 Training loss: 0.0000 Explore P: 0.1558\n",
      "Episode: 545 Total reward: 18.409515380859375 Training loss: 0.0000 Explore P: 0.1554\n",
      "Model Saved\n",
      "Episode: 546 Total reward: 386.9945983886719 Training loss: 0.0000 Explore P: 0.1547\n",
      "Episode: 547 Total reward: -8.969772338867188 Training loss: 0.0000 Explore P: 0.1544\n",
      "Episode: 548 Total reward: -29.808624267578125 Training loss: 0.0000 Explore P: 0.1540\n",
      "Episode: 549 Total reward: 48.90617370605469 Training loss: 0.0000 Explore P: 0.1537\n",
      "Episode: 550 Total reward: 142.1889190673828 Training loss: 0.0000 Explore P: 0.1531\n",
      "Model Saved\n",
      "Episode: 551 Total reward: -17.38336181640625 Training loss: 0.0000 Explore P: 0.1527\n",
      "Episode: 552 Total reward: 193.39920043945312 Training loss: 0.0000 Explore P: 0.1521\n",
      "Episode: 553 Total reward: 400.8721618652344 Training loss: 0.0000 Explore P: 0.1513\n",
      "Episode: 554 Total reward: -10.360031127929688 Training loss: 0.0000 Explore P: 0.1510\n",
      "Episode: 555 Total reward: 301.56907653808594 Training loss: 0.0000 Explore P: 0.1504\n",
      "Model Saved\n",
      "Episode: 556 Total reward: 161.4037322998047 Training loss: 0.0000 Explore P: 0.1499\n",
      "Episode: 557 Total reward: 399.14988708496094 Training loss: 0.0000 Explore P: 0.1492\n",
      "Episode: 558 Total reward: 88.99005126953125 Training loss: 0.0000 Explore P: 0.1488\n",
      "Episode: 559 Total reward: 39.04896545410156 Training loss: 0.0000 Explore P: 0.1485\n",
      "Episode: 560 Total reward: -22.837600708007812 Training loss: 0.0000 Explore P: 0.1482\n",
      "Model Saved\n",
      "Episode: 561 Total reward: 434.34312438964844 Training loss: 0.0000 Explore P: 0.1475\n",
      "Episode: 562 Total reward: 16.081268310546875 Training loss: 0.0000 Explore P: 0.1472\n",
      "Episode: 563 Total reward: 223.90814208984375 Training loss: 0.0000 Explore P: 0.1467\n",
      "Episode: 564 Total reward: 243.71749877929688 Training loss: 0.0000 Explore P: 0.1462\n",
      "Episode: 565 Total reward: -19.323013305664062 Training loss: 0.0000 Explore P: 0.1459\n",
      "Model Saved\n",
      "Episode: 566 Total reward: 248.2402801513672 Training loss: 0.0000 Explore P: 0.1453\n",
      "Episode: 567 Total reward: -11.357254028320312 Training loss: 0.0000 Explore P: 0.1450\n",
      "Episode: 568 Total reward: 313.76220703125 Training loss: 0.0000 Explore P: 0.1444\n",
      "Episode: 569 Total reward: 82.79191589355469 Training loss: 0.0000 Explore P: 0.1441\n",
      "Model updated\n",
      "Episode: 570 Total reward: 53.12306213378906 Training loss: 0.0000 Explore P: 0.1437\n",
      "Model Saved\n",
      "Episode: 571 Total reward: -13.780868530273438 Training loss: 0.0000 Explore P: 0.1434\n",
      "Episode: 572 Total reward: 436.94447326660156 Training loss: 0.0000 Explore P: 0.1427\n",
      "Episode: 573 Total reward: 213.6274871826172 Training loss: 0.0000 Explore P: 0.1422\n",
      "Episode: 574 Total reward: 371.9247589111328 Training loss: 0.0000 Explore P: 0.1415\n",
      "Episode: 575 Total reward: 70.855224609375 Training loss: 0.0000 Explore P: 0.1412\n",
      "Model Saved\n",
      "Episode: 576 Total reward: -36.11445617675781 Training loss: 0.0000 Explore P: 0.1409\n",
      "Episode: 577 Total reward: 203.367919921875 Training loss: 0.0000 Explore P: 0.1404\n",
      "Episode: 578 Total reward: -20.718551635742188 Training loss: 0.0000 Explore P: 0.1401\n",
      "Episode: 579 Total reward: 130.2945098876953 Training loss: 0.0000 Explore P: 0.1396\n",
      "Episode: 580 Total reward: 274.4277801513672 Training loss: 0.0000 Explore P: 0.1390\n",
      "Model Saved\n",
      "Episode: 581 Total reward: 316.9294738769531 Training loss: 0.0000 Explore P: 0.1385\n",
      "Episode: 582 Total reward: 125.69393920898438 Training loss: 0.0000 Explore P: 0.1380\n",
      "Episode: 583 Total reward: 64.2080078125 Training loss: 0.0000 Explore P: 0.1377\n",
      "Episode: 584 Total reward: -22.767364501953125 Training loss: 0.0000 Explore P: 0.1374\n",
      "Episode: 585 Total reward: 159.09133911132812 Training loss: 0.0000 Explore P: 0.1369\n",
      "Model Saved\n",
      "Episode: 586 Total reward: 144.15835571289062 Training loss: 0.0000 Explore P: 0.1364\n",
      "Episode: 587 Total reward: -38.40179443359375 Training loss: 0.0000 Explore P: 0.1361\n",
      "Episode: 588 Total reward: -6.812835693359375 Training loss: 0.0000 Explore P: 0.1358\n",
      "Episode: 589 Total reward: 33.253143310546875 Training loss: 0.0000 Explore P: 0.1355\n",
      "Episode: 590 Total reward: -35.21882629394531 Training loss: 0.0000 Explore P: 0.1352\n",
      "Model Saved\n",
      "Episode: 591 Total reward: 1.434539794921875 Training loss: 0.0000 Explore P: 0.1349\n",
      "Episode: 592 Total reward: 40.458831787109375 Training loss: 0.0000 Explore P: 0.1346\n",
      "Episode: 593 Total reward: 31.88873291015625 Training loss: 0.0000 Explore P: 0.1343\n",
      "Episode: 594 Total reward: 35.24858093261719 Training loss: 0.0000 Explore P: 0.1340\n",
      "Episode: 595 Total reward: 446.29869079589844 Training loss: 0.0000 Explore P: 0.1333\n",
      "Model Saved\n",
      "Episode: 596 Total reward: 103.28030395507812 Training loss: 0.0000 Explore P: 0.1329\n",
      "Episode: 597 Total reward: 83.36265563964844 Training loss: 0.0000 Explore P: 0.1324\n",
      "Episode: 598 Total reward: 118.62721252441406 Training loss: 0.0000 Explore P: 0.1319\n",
      "Episode: 599 Total reward: 383.02195739746094 Training loss: 0.0000 Explore P: 0.1313\n",
      "Episode: 600 Total reward: 195.54078674316406 Training loss: 0.0000 Explore P: 0.1309\n",
      "Model Saved\n",
      "Episode: 601 Total reward: 230.46246337890625 Training loss: 0.0000 Explore P: 0.1304\n",
      "Episode: 602 Total reward: 36.40449523925781 Training loss: 0.0000 Explore P: 0.1301\n",
      "Episode: 603 Total reward: -7.5462799072265625 Training loss: 0.0000 Explore P: 0.1298\n",
      "Episode: 604 Total reward: 216.8170928955078 Training loss: 0.0000 Explore P: 0.1293\n",
      "Episode: 605 Total reward: 98.47334289550781 Training loss: 0.0000 Explore P: 0.1289\n",
      "Model Saved\n",
      "Episode: 606 Total reward: 5.9727783203125 Training loss: 0.0000 Explore P: 0.1286\n",
      "Episode: 607 Total reward: -14.431686401367188 Training loss: 0.0000 Explore P: 0.1282\n",
      "Episode: 608 Total reward: 199.80279541015625 Training loss: 0.0000 Explore P: 0.1277\n",
      "Episode: 609 Total reward: 51.97613525390625 Training loss: 0.0000 Explore P: 0.1274\n",
      "Episode: 610 Total reward: 164.64488220214844 Training loss: 0.0000 Explore P: 0.1270\n",
      "Model Saved\n",
      "Episode: 611 Total reward: 372.0501708984375 Training loss: 0.0000 Explore P: 0.1264\n",
      "Episode: 612 Total reward: 163.8959503173828 Training loss: 0.0000 Explore P: 0.1259\n",
      "Episode: 613 Total reward: 9.550582885742188 Training loss: 0.0000 Explore P: 0.1257\n",
      "Episode: 614 Total reward: 223.556884765625 Training loss: 0.0000 Explore P: 0.1252\n",
      "Episode: 615 Total reward: 59.52418518066406 Training loss: 0.0000 Explore P: 0.1250\n",
      "Model Saved\n",
      "Episode: 616 Total reward: -68.12272644042969 Training loss: 0.0000 Explore P: 0.1247\n",
      "Episode: 617 Total reward: 38.11500549316406 Training loss: 0.0000 Explore P: 0.1244\n",
      "Episode: 618 Total reward: -18.201309204101562 Training loss: 0.0000 Explore P: 0.1242\n",
      "Episode: 619 Total reward: 83.48027038574219 Training loss: 0.0000 Explore P: 0.1239\n",
      "Episode: 620 Total reward: 219.93133544921875 Training loss: 0.0000 Explore P: 0.1234\n",
      "Model Saved\n",
      "Episode: 621 Total reward: 136.21128845214844 Training loss: 0.0000 Explore P: 0.1230\n",
      "Episode: 622 Total reward: 133.09210205078125 Training loss: 0.0000 Explore P: 0.1226\n",
      "Episode: 623 Total reward: -29.0328369140625 Training loss: 0.0000 Explore P: 0.1223\n",
      "Episode: 624 Total reward: 229.02613830566406 Training loss: 0.0000 Explore P: 0.1219\n",
      "Episode: 625 Total reward: 16.343490600585938 Training loss: 0.0000 Explore P: 0.1216\n",
      "Model Saved\n",
      "Episode: 626 Total reward: 86.7362060546875 Training loss: 0.0000 Explore P: 0.1212\n",
      "Episode: 627 Total reward: 86.10548400878906 Training loss: 0.0000 Explore P: 0.1209\n",
      "Episode: 628 Total reward: 27.117645263671875 Training loss: 0.0000 Explore P: 0.1206\n",
      "Episode: 629 Total reward: 145.9764862060547 Training loss: 0.0000 Explore P: 0.1202\n",
      "Episode: 630 Total reward: 206.5816650390625 Training loss: 0.0000 Explore P: 0.1197\n",
      "Model Saved\n",
      "Episode: 631 Total reward: 311.5232696533203 Training loss: 0.0000 Explore P: 0.1193\n",
      "Episode: 632 Total reward: 132.02098083496094 Training loss: 0.0000 Explore P: 0.1188\n",
      "Episode: 633 Total reward: 60.02455139160156 Training loss: 0.0000 Explore P: 0.1186\n",
      "Episode: 634 Total reward: 15.967056274414062 Training loss: 0.0000 Explore P: 0.1183\n",
      "Episode: 635 Total reward: 77.97808837890625 Training loss: 0.0000 Explore P: 0.1179\n",
      "Model Saved\n",
      "Episode: 636 Total reward: 68.79156494140625 Training loss: 0.0000 Explore P: 0.1176\n",
      "Episode: 637 Total reward: 364.65045166015625 Training loss: 0.0000 Explore P: 0.1172\n",
      "Episode: 638 Total reward: -64.70809936523438 Training loss: 0.0000 Explore P: 0.1170\n",
      "Episode: 639 Total reward: -54.68913269042969 Training loss: 0.0000 Explore P: 0.1167\n",
      "Episode: 640 Total reward: 83.10244750976562 Training loss: 0.0000 Explore P: 0.1164\n",
      "Model Saved\n",
      "Episode: 641 Total reward: 679.4777984619141 Training loss: 0.0000 Explore P: 0.1157\n",
      "Episode: 642 Total reward: 68.456787109375 Training loss: 0.0000 Explore P: 0.1154\n",
      "Episode: 643 Total reward: 70.68550109863281 Training loss: 0.0000 Explore P: 0.1152\n",
      "Episode: 644 Total reward: 371.5233154296875 Training loss: 0.0000 Explore P: 0.1146\n",
      "Episode: 645 Total reward: 5.1148223876953125 Training loss: 0.0000 Explore P: 0.1144\n",
      "Model Saved\n",
      "Episode: 646 Total reward: 59.12846374511719 Training loss: 0.0000 Explore P: 0.1140\n",
      "Episode: 647 Total reward: 388.9201354980469 Training loss: 0.0000 Explore P: 0.1134\n",
      "Episode: 648 Total reward: 40.81489562988281 Training loss: 0.0000 Explore P: 0.1132\n",
      "Episode: 649 Total reward: 278.33868408203125 Training loss: 0.0000 Explore P: 0.1127\n",
      "Episode: 650 Total reward: 7.8781585693359375 Training loss: 0.0000 Explore P: 0.1125\n",
      "Model Saved\n",
      "Episode: 651 Total reward: 245.3928985595703 Training loss: 0.0000 Explore P: 0.1121\n",
      "Episode: 652 Total reward: 87.54156494140625 Training loss: 0.0000 Explore P: 0.1118\n",
      "Episode: 653 Total reward: 19.308181762695312 Training loss: 0.0000 Explore P: 0.1116\n",
      "Episode: 654 Total reward: 326.8295440673828 Training loss: 0.0000 Explore P: 0.1112\n",
      "Episode: 655 Total reward: 41.29151916503906 Training loss: 0.0000 Explore P: 0.1110\n",
      "Model Saved\n",
      "Episode: 656 Total reward: 9.782485961914062 Training loss: 0.0000 Explore P: 0.1108\n",
      "Episode: 657 Total reward: 204.03334045410156 Training loss: 0.0000 Explore P: 0.1104\n",
      "Episode: 658 Total reward: 156.6107635498047 Training loss: 0.0000 Explore P: 0.1100\n",
      "Episode: 659 Total reward: -32.45130920410156 Training loss: 0.0000 Explore P: 0.1097\n",
      "Episode: 660 Total reward: 17.547134399414062 Training loss: 0.0000 Explore P: 0.1095\n",
      "Model Saved\n",
      "Episode: 661 Total reward: -16.964981079101562 Training loss: 0.0000 Explore P: 0.1092\n",
      "Episode: 662 Total reward: 217.21839904785156 Training loss: 0.0000 Explore P: 0.1088\n",
      "Episode: 663 Total reward: 65.16714477539062 Training loss: 0.0000 Explore P: 0.1085\n",
      "Episode: 664 Total reward: 220.44668579101562 Training loss: 0.0000 Explore P: 0.1081\n",
      "Episode: 665 Total reward: -11.290023803710938 Training loss: 0.0000 Explore P: 0.1078\n",
      "Model Saved\n",
      "Episode: 666 Total reward: -9.65216064453125 Training loss: 0.0000 Explore P: 0.1076\n",
      "Episode: 667 Total reward: 218.57101440429688 Training loss: 0.0000 Explore P: 0.1072\n",
      "Episode: 668 Total reward: 51.928070068359375 Training loss: 0.0000 Explore P: 0.1070\n",
      "Episode: 669 Total reward: 125.52033996582031 Training loss: 0.0000 Explore P: 0.1066\n",
      "Episode: 670 Total reward: 114.87812805175781 Training loss: 0.0000 Explore P: 0.1063\n",
      "Model Saved\n",
      "Episode: 671 Total reward: 137.2370147705078 Training loss: 0.0000 Explore P: 0.1059\n",
      "Episode: 672 Total reward: 202.74642944335938 Training loss: 0.0000 Explore P: 0.1056\n",
      "Episode: 673 Total reward: 97.71327209472656 Training loss: 0.0000 Explore P: 0.1052\n",
      "Episode: 674 Total reward: 265.8078308105469 Training loss: 0.0000 Explore P: 0.1048\n",
      "Episode: 675 Total reward: 259.1589660644531 Training loss: 0.0000 Explore P: 0.1044\n",
      "Model Saved\n",
      "Episode: 676 Total reward: 517.1165771484375 Training loss: 0.0000 Explore P: 0.1040\n",
      "Episode: 677 Total reward: -48.39671325683594 Training loss: 0.0000 Explore P: 0.1037\n",
      "Episode: 678 Total reward: 420.54750061035156 Training loss: 0.0000 Explore P: 0.1033\n",
      "Episode: 679 Total reward: 29.849441528320312 Training loss: 0.0000 Explore P: 0.1030\n",
      "Episode: 680 Total reward: -6.6875 Training loss: 0.0000 Explore P: 0.1028\n",
      "Model Saved\n",
      "Episode: 681 Total reward: -5.7578887939453125 Training loss: 0.0000 Explore P: 0.1026\n",
      "Episode: 682 Total reward: 338.5382385253906 Training loss: 0.0000 Explore P: 0.1021\n",
      "Episode: 683 Total reward: 301.03466796875 Training loss: 0.0000 Explore P: 0.1017\n",
      "Episode: 684 Total reward: 102.79975891113281 Training loss: 0.0000 Explore P: 0.1013\n",
      "Episode: 685 Total reward: -54.22088623046875 Training loss: 0.0000 Explore P: 0.1011\n",
      "Model Saved\n",
      "Episode: 686 Total reward: -41.78791809082031 Training loss: 0.0000 Explore P: 0.1009\n",
      "Episode: 687 Total reward: -18.764328002929688 Training loss: 0.0000 Explore P: 0.1007\n",
      "Episode: 688 Total reward: 43.71185302734375 Training loss: 0.0000 Explore P: 0.1005\n",
      "Episode: 689 Total reward: 143.98681640625 Training loss: 0.0000 Explore P: 0.1001\n",
      "Episode: 690 Total reward: 28.082962036132812 Training loss: 0.0000 Explore P: 0.0999\n",
      "Model Saved\n",
      "Episode: 691 Total reward: 4.8438262939453125 Training loss: 0.0000 Explore P: 0.0997\n",
      "Episode: 692 Total reward: 97.57667541503906 Training loss: 0.0000 Explore P: 0.0995\n",
      "Episode: 693 Total reward: 372.15342712402344 Training loss: 0.0000 Explore P: 0.0990\n",
      "Episode: 694 Total reward: -18.550949096679688 Training loss: 0.0000 Explore P: 0.0988\n",
      "Episode: 695 Total reward: -3.8744659423828125 Training loss: 0.0000 Explore P: 0.0986\n",
      "Model Saved\n",
      "Episode: 696 Total reward: 63.607269287109375 Training loss: 0.0000 Explore P: 0.0984\n",
      "Episode: 697 Total reward: 319.2142028808594 Training loss: 0.0000 Explore P: 0.0980\n",
      "Episode: 698 Total reward: 709.6719512939453 Training loss: 0.0000 Explore P: 0.0974\n",
      "Episode: 699 Total reward: -25.51165771484375 Training loss: 0.0000 Explore P: 0.0972\n",
      "Episode: 700 Total reward: -16.140670776367188 Training loss: 0.0000 Explore P: 0.0970\n",
      "Model Saved\n",
      "Episode: 701 Total reward: -49.027496337890625 Training loss: 0.0000 Explore P: 0.0968\n",
      "Episode: 702 Total reward: 396.12396240234375 Training loss: 0.0000 Explore P: 0.0964\n",
      "Episode: 703 Total reward: 63.58476257324219 Training loss: 0.0000 Explore P: 0.0961\n",
      "Episode: 704 Total reward: -25.723114013671875 Training loss: 0.0000 Explore P: 0.0959\n",
      "Episode: 705 Total reward: 457.4205017089844 Training loss: 0.0000 Explore P: 0.0955\n",
      "Model Saved\n",
      "Episode: 706 Total reward: 715.9599151611328 Training loss: 0.0000 Explore P: 0.0948\n",
      "Episode: 707 Total reward: -23.140106201171875 Training loss: 0.0000 Explore P: 0.0946\n",
      "Episode: 708 Total reward: 320.1207275390625 Training loss: 0.0000 Explore P: 0.0942\n",
      "Episode: 709 Total reward: 193.59361267089844 Training loss: 0.0000 Explore P: 0.0939\n",
      "Episode: 710 Total reward: -7.1132049560546875 Training loss: 0.0000 Explore P: 0.0937\n",
      "Model Saved\n",
      "Episode: 711 Total reward: -6.7398223876953125 Training loss: 0.0000 Explore P: 0.0935\n",
      "Episode: 712 Total reward: 308.6432342529297 Training loss: 0.0000 Explore P: 0.0932\n",
      "Episode: 713 Total reward: 14.358245849609375 Training loss: 0.0000 Explore P: 0.0930\n",
      "Episode: 714 Total reward: -21.700714111328125 Training loss: 0.0000 Explore P: 0.0928\n",
      "Episode: 715 Total reward: 214.92250061035156 Training loss: 0.0000 Explore P: 0.0925\n",
      "Model Saved\n",
      "Episode: 716 Total reward: 259.07676696777344 Training loss: 0.0000 Explore P: 0.0922\n",
      "Episode: 717 Total reward: -29.834487915039062 Training loss: 0.0000 Explore P: 0.0920\n",
      "Episode: 718 Total reward: 128.5249786376953 Training loss: 0.0000 Explore P: 0.0917\n",
      "Episode: 719 Total reward: 476.218994140625 Training loss: 0.0000 Explore P: 0.0912\n",
      "Model updated\n",
      "Episode: 720 Total reward: 166.56935119628906 Training loss: 0.0000 Explore P: 0.0909\n",
      "Model Saved\n",
      "Episode: 721 Total reward: 179.6253662109375 Training loss: 0.0000 Explore P: 0.0906\n",
      "Episode: 722 Total reward: 88.25747680664062 Training loss: 0.0000 Explore P: 0.0904\n",
      "Episode: 723 Total reward: 440.23899841308594 Training loss: 0.0000 Explore P: 0.0900\n",
      "Episode: 724 Total reward: 31.89788818359375 Training loss: 0.0000 Explore P: 0.0898\n",
      "Episode: 725 Total reward: 2.80999755859375 Training loss: 0.0000 Explore P: 0.0896\n",
      "Model Saved\n",
      "Episode: 726 Total reward: 94.67564392089844 Training loss: 0.0000 Explore P: 0.0893\n",
      "Episode: 727 Total reward: 26.321807861328125 Training loss: 0.0000 Explore P: 0.0891\n",
      "Episode: 728 Total reward: 203.10438537597656 Training loss: 0.0000 Explore P: 0.0888\n",
      "Episode: 729 Total reward: 62.56788635253906 Training loss: 0.0000 Explore P: 0.0886\n",
      "Episode: 730 Total reward: 57.262420654296875 Training loss: 0.0000 Explore P: 0.0884\n",
      "Model Saved\n",
      "Episode: 731 Total reward: 45.93156433105469 Training loss: 0.0000 Explore P: 0.0882\n",
      "Episode: 732 Total reward: 112.73837280273438 Training loss: 0.0000 Explore P: 0.0880\n",
      "Episode: 733 Total reward: -25.312484741210938 Training loss: 0.0000 Explore P: 0.0878\n",
      "Episode: 734 Total reward: -27.0626220703125 Training loss: 0.0000 Explore P: 0.0876\n",
      "Episode: 735 Total reward: 127.65042114257812 Training loss: 0.0000 Explore P: 0.0874\n",
      "Model Saved\n",
      "Episode: 736 Total reward: 348.70606994628906 Training loss: 0.0000 Explore P: 0.0871\n",
      "Episode: 737 Total reward: 154.0493621826172 Training loss: 0.0000 Explore P: 0.0868\n",
      "Episode: 738 Total reward: 276.64939880371094 Training loss: 0.0000 Explore P: 0.0865\n",
      "Episode: 739 Total reward: 308.7714080810547 Training loss: 0.0000 Explore P: 0.0862\n",
      "Episode: 740 Total reward: 429.3704376220703 Training loss: 0.0000 Explore P: 0.0858\n",
      "Model Saved\n",
      "Episode: 741 Total reward: 185.5393829345703 Training loss: 0.0000 Explore P: 0.0855\n",
      "Episode: 742 Total reward: 98.91709899902344 Training loss: 0.0000 Explore P: 0.0853\n",
      "Episode: 743 Total reward: 153.25216674804688 Training loss: 0.0000 Explore P: 0.0850\n",
      "Episode: 744 Total reward: 205.30010986328125 Training loss: 0.0000 Explore P: 0.0847\n",
      "Episode: 745 Total reward: 74.55853271484375 Training loss: 0.0000 Explore P: 0.0846\n",
      "Model Saved\n",
      "Episode: 746 Total reward: -13.082687377929688 Training loss: 0.0000 Explore P: 0.0844\n",
      "Episode: 747 Total reward: -30.535614013671875 Training loss: 0.0000 Explore P: 0.0842\n",
      "Episode: 748 Total reward: 96.95932006835938 Training loss: 0.0000 Explore P: 0.0839\n",
      "Episode: 749 Total reward: 21.943267822265625 Training loss: 0.0000 Explore P: 0.0838\n",
      "Episode: 750 Total reward: -13.884353637695312 Training loss: 0.0000 Explore P: 0.0836\n",
      "Model Saved\n",
      "Episode: 751 Total reward: 116.55259704589844 Training loss: 0.0000 Explore P: 0.0833\n",
      "Episode: 752 Total reward: -9.980316162109375 Training loss: 0.0000 Explore P: 0.0832\n",
      "Episode: 753 Total reward: 50.48677062988281 Training loss: 0.0000 Explore P: 0.0830\n",
      "Episode: 754 Total reward: 427.8959045410156 Training loss: 0.0000 Explore P: 0.0826\n",
      "Episode: 755 Total reward: 152.59652709960938 Training loss: 0.0000 Explore P: 0.0823\n",
      "Model Saved\n",
      "Episode: 756 Total reward: 214.9467010498047 Training loss: 0.0000 Explore P: 0.0820\n",
      "Episode: 757 Total reward: 252.64212036132812 Training loss: 0.0000 Explore P: 0.0817\n",
      "Episode: 758 Total reward: 144.43185424804688 Training loss: 0.0000 Explore P: 0.0815\n",
      "Episode: 759 Total reward: 91.29649353027344 Training loss: 0.0000 Explore P: 0.0813\n",
      "Episode: 760 Total reward: 444.2386016845703 Training loss: 0.0000 Explore P: 0.0809\n",
      "Model Saved\n",
      "Episode: 761 Total reward: 344.2602844238281 Training loss: 0.0000 Explore P: 0.0806\n",
      "Episode: 762 Total reward: 283.89817810058594 Training loss: 0.0000 Explore P: 0.0803\n",
      "Episode: 763 Total reward: -27.287704467773438 Training loss: 0.0000 Explore P: 0.0802\n",
      "Episode: 764 Total reward: 51.47119140625 Training loss: 0.0000 Explore P: 0.0800\n",
      "Episode: 765 Total reward: 64.67291259765625 Training loss: 0.0000 Explore P: 0.0798\n",
      "Model Saved\n",
      "Episode: 766 Total reward: 78.98492431640625 Training loss: 0.0000 Explore P: 0.0796\n",
      "Episode: 767 Total reward: 138.55685424804688 Training loss: 0.0000 Explore P: 0.0794\n",
      "Episode: 768 Total reward: 422.22039794921875 Training loss: 0.0000 Explore P: 0.0791\n",
      "Episode: 769 Total reward: 51.55158996582031 Training loss: 0.0000 Explore P: 0.0789\n",
      "Episode: 770 Total reward: -28.7603759765625 Training loss: 0.0000 Explore P: 0.0788\n",
      "Model Saved\n",
      "Episode: 771 Total reward: 17.614059448242188 Training loss: 0.0000 Explore P: 0.0786\n",
      "Episode: 772 Total reward: 125.139892578125 Training loss: 0.0000 Explore P: 0.0783\n",
      "Episode: 773 Total reward: 180.5050811767578 Training loss: 0.0000 Explore P: 0.0780\n",
      "Episode: 774 Total reward: 34.82539367675781 Training loss: 0.0000 Explore P: 0.0779\n",
      "Episode: 775 Total reward: 354.20477294921875 Training loss: 0.0000 Explore P: 0.0776\n",
      "Model Saved\n",
      "Episode: 776 Total reward: 380.9197692871094 Training loss: 0.0000 Explore P: 0.0773\n",
      "Episode: 777 Total reward: 380.5569610595703 Training loss: 0.0000 Explore P: 0.0770\n",
      "Episode: 778 Total reward: 216.9456787109375 Training loss: 0.0000 Explore P: 0.0768\n",
      "Episode: 779 Total reward: 57.40818786621094 Training loss: 0.0000 Explore P: 0.0766\n",
      "Episode: 780 Total reward: 133.5462188720703 Training loss: 0.0000 Explore P: 0.0764\n",
      "Model Saved\n",
      "Episode: 781 Total reward: 81.7374267578125 Training loss: 0.0000 Explore P: 0.0761\n",
      "Episode: 782 Total reward: 373.46861267089844 Training loss: 0.0000 Explore P: 0.0758\n",
      "Episode: 783 Total reward: 263.0819396972656 Training loss: 0.0000 Explore P: 0.0756\n",
      "Episode: 784 Total reward: -27.215103149414062 Training loss: 0.0000 Explore P: 0.0754\n",
      "Episode: 785 Total reward: 18.92572021484375 Training loss: 0.0000 Explore P: 0.0753\n",
      "Model Saved\n",
      "Episode: 786 Total reward: 420.1455993652344 Training loss: 0.0000 Explore P: 0.0750\n",
      "Episode: 787 Total reward: 92.72654724121094 Training loss: 0.0000 Explore P: 0.0748\n",
      "Episode: 788 Total reward: -44.932525634765625 Training loss: 0.0000 Explore P: 0.0747\n",
      "Episode: 789 Total reward: 270.6167755126953 Training loss: 0.0000 Explore P: 0.0744\n",
      "Episode: 790 Total reward: 349.9538116455078 Training loss: 0.0000 Explore P: 0.0741\n",
      "Model Saved\n",
      "Episode: 791 Total reward: 327.79742431640625 Training loss: 0.0000 Explore P: 0.0739\n",
      "Episode: 792 Total reward: 76.85960388183594 Training loss: 0.0000 Explore P: 0.0737\n",
      "Episode: 793 Total reward: -19.7744140625 Training loss: 0.0000 Explore P: 0.0736\n",
      "Episode: 794 Total reward: -15.581649780273438 Training loss: 0.0000 Explore P: 0.0734\n",
      "Episode: 795 Total reward: 391.8078918457031 Training loss: 0.0000 Explore P: 0.0731\n",
      "Model Saved\n",
      "Episode: 796 Total reward: 294.3433837890625 Training loss: 0.0000 Explore P: 0.0728\n",
      "Episode: 797 Total reward: -0.4932861328125 Training loss: 0.0000 Explore P: 0.0726\n",
      "Episode: 798 Total reward: 45.20118713378906 Training loss: 0.0000 Explore P: 0.0725\n",
      "Episode: 799 Total reward: -26.567977905273438 Training loss: 0.0000 Explore P: 0.0724\n",
      "Episode: 800 Total reward: -84.33894348144531 Training loss: 0.0000 Explore P: 0.0722\n",
      "Model Saved\n",
      "Episode: 801 Total reward: 17.469650268554688 Training loss: 0.0000 Explore P: 0.0721\n",
      "Episode: 802 Total reward: 445.17640686035156 Training loss: 0.0000 Explore P: 0.0718\n",
      "Episode: 803 Total reward: 36.51092529296875 Training loss: 0.0000 Explore P: 0.0717\n",
      "Episode: 804 Total reward: 69.29350280761719 Training loss: 0.0000 Explore P: 0.0714\n",
      "Episode: 805 Total reward: 399.14442443847656 Training loss: 0.0000 Explore P: 0.0712\n",
      "Model Saved\n",
      "Episode: 806 Total reward: 389.865966796875 Training loss: 0.0000 Explore P: 0.0708\n",
      "Episode: 807 Total reward: -20.145034790039062 Training loss: 0.0000 Explore P: 0.0707\n",
      "Episode: 808 Total reward: 85.28355407714844 Training loss: 0.0000 Explore P: 0.0705\n",
      "Episode: 809 Total reward: 295.9600067138672 Training loss: 0.0000 Explore P: 0.0703\n",
      "Episode: 810 Total reward: 179.5196990966797 Training loss: 0.0000 Explore P: 0.0701\n",
      "Model Saved\n",
      "Episode: 811 Total reward: 46.86163330078125 Training loss: 0.0000 Explore P: 0.0700\n",
      "Episode: 812 Total reward: 29.618072509765625 Training loss: 0.0000 Explore P: 0.0698\n",
      "Episode: 813 Total reward: 217.19418334960938 Training loss: 0.0000 Explore P: 0.0696\n",
      "Episode: 814 Total reward: -50.00965881347656 Training loss: 0.0000 Explore P: 0.0695\n",
      "Episode: 815 Total reward: -18.352630615234375 Training loss: 0.0000 Explore P: 0.0693\n",
      "Model Saved\n",
      "Episode: 816 Total reward: 394.55259704589844 Training loss: 0.0000 Explore P: 0.0690\n",
      "Episode: 817 Total reward: 86.43630981445312 Training loss: 0.0000 Explore P: 0.0689\n",
      "Episode: 818 Total reward: 293.9888916015625 Training loss: 0.0000 Explore P: 0.0686\n",
      "Episode: 819 Total reward: 177.46322631835938 Training loss: 0.0000 Explore P: 0.0684\n",
      "Episode: 820 Total reward: 482.64915466308594 Training loss: 0.0000 Explore P: 0.0681\n",
      "Model Saved\n",
      "Episode: 821 Total reward: 60.904754638671875 Training loss: 0.0000 Explore P: 0.0679\n",
      "Episode: 822 Total reward: -13.917312622070312 Training loss: 0.0000 Explore P: 0.0678\n",
      "Episode: 823 Total reward: 269.7703552246094 Training loss: 0.0000 Explore P: 0.0675\n",
      "Episode: 824 Total reward: 24.031295776367188 Training loss: 0.0000 Explore P: 0.0674\n",
      "Episode: 825 Total reward: 319.8516845703125 Training loss: 0.0000 Explore P: 0.0671\n",
      "Model Saved\n",
      "Episode: 826 Total reward: 156.4409942626953 Training loss: 0.0000 Explore P: 0.0669\n",
      "Episode: 827 Total reward: 390.5139923095703 Training loss: 0.0000 Explore P: 0.0666\n",
      "Episode: 828 Total reward: 92.54705810546875 Training loss: 0.0000 Explore P: 0.0665\n",
      "Episode: 829 Total reward: 295.4375 Training loss: 0.0000 Explore P: 0.0663\n",
      "Episode: 830 Total reward: 223.03973388671875 Training loss: 0.0000 Explore P: 0.0661\n",
      "Model Saved\n",
      "Episode: 831 Total reward: 38.292755126953125 Training loss: 0.0000 Explore P: 0.0660\n",
      "Episode: 832 Total reward: 89.48539733886719 Training loss: 0.0000 Explore P: 0.0658\n",
      "Episode: 833 Total reward: 291.1689147949219 Training loss: 0.0000 Explore P: 0.0656\n",
      "Episode: 834 Total reward: -18.52801513671875 Training loss: 0.0000 Explore P: 0.0655\n",
      "Episode: 835 Total reward: 120.41555786132812 Training loss: 0.0000 Explore P: 0.0653\n",
      "Model Saved\n",
      "Episode: 836 Total reward: -14.030563354492188 Training loss: 0.0000 Explore P: 0.0651\n",
      "Episode: 837 Total reward: -19.144989013671875 Training loss: 0.0000 Explore P: 0.0650\n",
      "Episode: 838 Total reward: 244.1021728515625 Training loss: 0.0000 Explore P: 0.0648\n",
      "Episode: 839 Total reward: 55.483245849609375 Training loss: 0.0000 Explore P: 0.0646\n",
      "Episode: 840 Total reward: 529.2422027587891 Training loss: 0.0000 Explore P: 0.0643\n",
      "Model Saved\n",
      "Episode: 841 Total reward: 43.13055419921875 Training loss: 0.0000 Explore P: 0.0642\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, \"./models/model.ckpt\")\n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase the C step\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # With œµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((1,120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ### DOUBLE DQN Logic\n",
    "                # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                \n",
    "                # Get Q values for next_state \n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Calculate Qtarget for all actions that state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # We got a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        # Take the Qtarget for action a'\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                \n",
    "                \n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "xQtNwGaoFQ4y",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Episode: 0 Total reward: -89.26519775390625 Training loss: 0.6859 Explore P: 0.9919\n",
    "Model Saved\n",
    "Episode: 1 Total reward: -115.13249206542969 Training loss: 1.0044 Explore P: 0.9872\n",
    "Episode: 2 Total reward: -88.6678466796875 Training loss: 30.3196 Explore P: 0.9760\n",
    "Episode: 3 Total reward: -79.75584411621094 Training loss: 0.4285 Explore P: 0.9687\n",
    "Episode: 4 Total reward: -112.888916015625 Training loss: 17.6260 Explore P: 0.9616\n",
    "Episode: 5 Total reward: -72.01809692382812 Training loss: 0.3325 Explore P: 0.9566\n",
    "Model Saved\n",
    "Episode: 6 Total reward: -91.27947998046875 Training loss: 11.2775 Explore P: 0.9489\n",
    "Episode: 7 Total reward: -96.70150756835938 Training loss: 0.8339 Explore P: 0.9412\n",
    "Episode: 8 Total reward: -89.98007202148438 Training loss: 10.4413 Explore P: 0.9368\n",
    "Episode: 9 Total reward: -78.67872619628906 Training loss: 13.1729 Explore P: 0.9292\n",
    "Episode: 10 Total reward: -114.67742919921875 Training loss: 30.3574 Explore P: 0.9192\n",
    "Model Saved\n",
    "Episode: 11 Total reward: -96.40922546386719 Training loss: 1.3338 Explore P: 0.9128\n",
    "Episode: 12 Total reward: -55.94306945800781 Training loss: 1.0281 Explore P: 0.9030\n",
    "Episode: 13 Total reward: -90.61083984375 Training loss: 17.3349 Explore P: 0.8908\n",
    "Episode: 14 Total reward: -110.54817199707031 Training loss: 11.5495 Explore P: 0.8845\n",
    "Episode: 15 Total reward: -86.7437744140625 Training loss: 8.5952 Explore P: 0.8773\n",
    "Model Saved\n",
    "Episode: 16 Total reward: -108.89964294433594 Training loss: 1.6884 Explore P: 0.8730\n",
    "Episode: 17 Total reward: -59.06085205078125 Training loss: 1.4325 Explore P: 0.8659\n",
    "Episode: 18 Total reward: -102.50300598144531 Training loss: 1.0175 Explore P: 0.8593\n",
    "Episode: 19 Total reward: -100.752685546875 Training loss: 7.5007 Explore P: 0.8556\n",
    "Episode: 20 Total reward: -111.81524658203125 Training loss: 5.0652 Explore P: 0.8487\n",
    "Model Saved\n",
    "Episode: 21 Total reward: -104.21478271484375 Training loss: 1.5522 Explore P: 0.8395\n",
    "Episode: 22 Total reward: -112.78564453125 Training loss: 5.5151 Explore P: 0.8359\n",
    "Episode: 23 Total reward: -82.05340576171875 Training loss: 1.2886 Explore P: 0.8292\n",
    "Episode: 24 Total reward: -111.15492248535156 Training loss: 0.5443 Explore P: 0.8225\n",
    "Episode: 25 Total reward: -73.76707458496094 Training loss: 1.5218 Explore P: 0.8156\n",
    "Model Saved\n",
    "Episode: 26 Total reward: -93.35487365722656 Training loss: 0.9257 Explore P: 0.8088\n",
    "Episode: 27 Total reward: -104.47758483886719 Training loss: 2.1761 Explore P: 0.8023\n",
    "Episode: 28 Total reward: -81.60890197753906 Training loss: 0.5991 Explore P: 0.7958\n",
    "Episode: 29 Total reward: -100.63589477539062 Training loss: 1.7395 Explore P: 0.7894\n",
    "Episode: 30 Total reward: -88.62884521484375 Training loss: 8.3556 Explore P: 0.7830\n",
    "Model Saved\n",
    "Episode: 31 Total reward: -105.23612976074219 Training loss: 0.8298 Explore P: 0.7767\n",
    "Episode: 32 Total reward: -111.5128173828125 Training loss: 0.6878 Explore P: 0.7711\n",
    "Episode: 33 Total reward: -107.63644409179688 Training loss: 0.7860 Explore P: 0.7651\n",
    "Episode: 34 Total reward: -99.78999328613281 Training loss: 1.9388 Explore P: 0.7567\n",
    "Episode: 35 Total reward: -107.68731689453125 Training loss: 3.5948 Explore P: 0.7481\n",
    "Model Saved\n",
    "Episode: 36 Total reward: -112.137451171875 Training loss: 0.4563 Explore P: 0.7421\n",
    "Episode: 37 Total reward: -50.57890319824219 Training loss: 0.5308 Explore P: 0.7361\n",
    "Episode: 38 Total reward: -73.00382995605469 Training loss: 1.9759 Explore P: 0.7302\n",
    "Episode: 39 Total reward: -80.82208251953125 Training loss: 0.2969 Explore P: 0.7243\n",
    "Episode: 40 Total reward: -97.41578674316406 Training loss: 16.1484 Explore P: 0.7185\n",
    "Model Saved\n",
    "Episode: 41 Total reward: -77.568115234375 Training loss: 0.2420 Explore P: 0.7128\n",
    "Episode: 42 Total reward: -103.93637084960938 Training loss: 0.1838 Explore P: 0.7026\n",
    "Episode: 43 Total reward: -81.61286926269531 Training loss: 0.3259 Explore P: 0.6948\n",
    "Episode: 44 Total reward: -91.02716064453125 Training loss: 0.3337 Explore P: 0.6859\n",
    "Episode: 45 Total reward: -98.70729064941406 Training loss: 2.1673 Explore P: 0.6804\n",
    "Model Saved\n",
    "Episode: 46 Total reward: -115.98574829101562 Training loss: 14.9863 Explore P: 0.6726\n",
    "Episode: 47 Total reward: -100.81024169921875 Training loss: 2.0342 Explore P: 0.6654\n",
    "Episode: 48 Total reward: -60.25152587890625 Training loss: 0.2753 Explore P: 0.6569\n",
    "Episode: 49 Total reward: -67.41098022460938 Training loss: 0.3018 Explore P: 0.6486\n",
    "Episode: 50 Total reward: -105.46267700195312 Training loss: 1.0995 Explore P: 0.6413\n",
    "Model Saved\n",
    "Episode: 51 Total reward: -73.07460021972656 Training loss: 0.1813 Explore P: 0.6362\n",
    "Episode: 52 Total reward: -96.30844116210938 Training loss: 0.2939 Explore P: 0.6310\n",
    "Episode: 53 Total reward: -94.21073913574219 Training loss: 0.4776 Explore P: 0.6284\n",
    "Episode: 54 Total reward: -65.328125 Training loss: 0.2104 Explore P: 0.6233\n",
    "Episode: 55 Total reward: -66.21479797363281 Training loss: 3.2012 Explore P: 0.6183\n",
    "Model Saved\n",
    "Episode: 56 Total reward: -94.83515930175781 Training loss: 0.5179 Explore P: 0.6136\n",
    "Episode: 57 Total reward: -92.63566589355469 Training loss: 7.6108 Explore P: 0.6068\n",
    "Episode: 58 Total reward: -114.22836303710938 Training loss: 0.1981 Explore P: 0.5979\n",
    "Episode: 59 Total reward: -109.301025390625 Training loss: 0.1633 Explore P: 0.5931\n",
    "Episode: 60 Total reward: -69.18382263183594 Training loss: 0.3027 Explore P: 0.5883\n",
    "Model Saved\n",
    "Episode: 61 Total reward: -96.5882568359375 Training loss: 0.2388 Explore P: 0.5856\n",
    "Episode: 62 Total reward: -115.95585632324219 Training loss: 0.2598 Explore P: 0.5815\n",
    "Episode: 63 Total reward: -91.42893981933594 Training loss: 3.1792 Explore P: 0.5768\n",
    "Episode: 64 Total reward: -78.47196960449219 Training loss: 0.1737 Explore P: 0.5722\n",
    "Episode: 65 Total reward: -33.51860046386719 Training loss: 16.5782 Explore P: 0.5676\n",
    "Model Saved\n",
    "Episode: 66 Total reward: -52.46026611328125 Training loss: 0.7277 Explore P: 0.5630\n",
    "Episode: 67 Total reward: -104.60054016113281 Training loss: 0.1622 Explore P: 0.5585\n",
    "Episode: 68 Total reward: -77.99497985839844 Training loss: 2.5138 Explore P: 0.5540\n",
    "Episode: 69 Total reward: -54.47041320800781 Training loss: 0.1590 Explore P: 0.5496\n",
    "Episode: 70 Total reward: -63.22991943359375 Training loss: 0.1965 Explore P: 0.5452\n",
    "Model Saved\n",
    "Episode: 71 Total reward: -87.78546142578125 Training loss: 0.3122 Explore P: 0.5375\n",
    "Episode: 72 Total reward: -96.14764404296875 Training loss: 0.1515 Explore P: 0.5351\n",
    "Episode: 73 Total reward: -69.32623291015625 Training loss: 2.8430 Explore P: 0.5308\n",
    "Episode: 74 Total reward: -13.840484619140625 Training loss: 0.2721 Explore P: 0.5266\n",
    "Episode: 75 Total reward: -89.6734619140625 Training loss: 0.1506 Explore P: 0.5213\n",
    "Model Saved\n",
    "Episode: 76 Total reward: -64.53419494628906 Training loss: 1.8367 Explore P: 0.5171\n",
    "Episode: 77 Total reward: -106.41300964355469 Training loss: 0.3183 Explore P: 0.5072\n",
    "Episode: 78 Total reward: -50.4837646484375 Training loss: 0.2255 Explore P: 0.5033\n",
    "Episode: 79 Total reward: -34.91241455078125 Training loss: 0.1923 Explore P: 0.4976\n",
    "Episode: 80 Total reward: -115.21119689941406 Training loss: 0.1336 Explore P: 0.4950\n",
    "Model Saved\n",
    "Episode: 81 Total reward: -73.21771240234375 Training loss: 0.1376 Explore P: 0.4911\n",
    "Episode: 82 Total reward: -62.74360656738281 Training loss: 0.6687 Explore P: 0.4871\n",
    "Episode: 83 Total reward: -15.30194091796875 Training loss: 0.1503 Explore P: 0.4778\n",
    "Episode: 84 Total reward: -74.79470825195312 Training loss: 0.1727 Explore P: 0.4740\n",
    "Episode: 85 Total reward: -54.167205810546875 Training loss: 0.1432 Explore P: 0.4702\n",
    "Model Saved\n",
    "Episode: 86 Total reward: -62.83433532714844 Training loss: 0.1632 Explore P: 0.4665\n",
    "Episode: 87 Total reward: -82.97991943359375 Training loss: 0.1923 Explore P: 0.4644\n",
    "Episode: 88 Total reward: -72.07733154296875 Training loss: 0.2274 Explore P: 0.4607\n",
    "Episode: 89 Total reward: -55.19401550292969 Training loss: 0.1261 Explore P: 0.4570\n",
    "Episode: 90 Total reward: -76.98689270019531 Training loss: 0.7601 Explore P: 0.4505\n",
    "Model Saved\n",
    "Episode: 91 Total reward: -65.32528686523438 Training loss: 0.3138 Explore P: 0.4469\n",
    "Episode: 92 Total reward: -50.588714599609375 Training loss: 0.2203 Explore P: 0.4435\n",
    "Episode: 93 Total reward: -70.39730834960938 Training loss: 1.2486 Explore P: 0.4415\n",
    "\n",
    "Episode: 94 Total reward: 70.74258422851562 Training loss: 0.4045 Explore P: 0.4366\n",
    "Episode: 95 Total reward: -11.190460205078125 Training loss: 0.2244 Explore P: 0.4331\n",
    "Model Saved\n",
    "Episode: 96 Total reward: -22.803070068359375 Training loss: 0.4332 Explore P: 0.4297\n",
    "Episode: 97 Total reward: -43.600616455078125 Training loss: 2.4079 Explore P: 0.4265\n",
    "Episode: 98 Total reward: -74.661376953125 Training loss: 0.3113 Explore P: 0.4246\n",
    "Episode: 99 Total reward: -32.23060607910156 Training loss: 0.1899 Explore P: 0.4212\n",
    "Episode: 100 Total reward: -66.32485961914062 Training loss: 0.1400 Explore P: 0.4167\n",
    "Model Saved\n",
    "Episode: 101 Total reward: -15.644882202148438 Training loss: 0.0826 Explore P: 0.4134\n",
    "Episode: 102 Total reward: 44.1182861328125 Training loss: 0.1348 Explore P: 0.4101\n",
    "Episode: 103 Total reward: -61.74578857421875 Training loss: 0.6734 Explore P: 0.4058\n",
    "Episode: 104 Total reward: -87.16415405273438 Training loss: 0.2358 Explore P: 0.4026\n",
    "Episode: 105 Total reward: -90.69143676757812 Training loss: 0.4390 Explore P: 0.3939\n",
    "Model Saved\n",
    "Episode: 106 Total reward: -56.23359680175781 Training loss: 0.1456 Explore P: 0.3908\n",
    "Episode: 107 Total reward: -41.05461120605469 Training loss: 0.9647 Explore P: 0.3877\n",
    "Episode: 108 Total reward: -1.7525482177734375 Training loss: 0.4109 Explore P: 0.3846\n",
    "Episode: 109 Total reward: -37.95100402832031 Training loss: 0.2784 Explore P: 0.3815\n",
    "Episode: 110 Total reward: -71.89024353027344 Training loss: 0.1012 Explore P: 0.3786\n",
    "Model Saved\n",
    "Episode: 111 Total reward: -72.90853881835938 Training loss: 1.4025 Explore P: 0.3756\n",
    "Model updated\n",
    "Episode: 112 Total reward: -56.199127197265625 Training loss: 7.5684 Explore P: 0.3727\n",
    "Episode: 113 Total reward: -77.53300476074219 Training loss: 3.6123 Explore P: 0.3698\n",
    "Episode: 114 Total reward: -50.253692626953125 Training loss: 6.0007 Explore P: 0.3668\n",
    "Episode: 115 Total reward: 18.208023071289062 Training loss: 6.2701 Explore P: 0.3639\n",
    "Model Saved\n",
    "Episode: 116 Total reward: -74.686767578125 Training loss: 7.9382 Explore P: 0.3610\n",
    "Episode: 117 Total reward: -76.70317077636719 Training loss: 3.9754 Explore P: 0.3593\n",
    "Episode: 118 Total reward: 18.843551635742188 Training loss: 1.0298 Explore P: 0.3554\n",
    "Episode: 119 Total reward: 1.3499298095703125 Training loss: 1.5573 Explore P: 0.3525\n",
    "Episode: 120 Total reward: -0.566131591796875 Training loss: 0.4084 Explore P: 0.3497\n",
    "Model Saved\n",
    "Episode: 121 Total reward: 20.053070068359375 Training loss: 0.6762 Explore P: 0.3470\n",
    "Episode: 122 Total reward: -79.74948120117188 Training loss: 0.5085 Explore P: 0.3443\n",
    "Episode: 123 Total reward: -68.07794189453125 Training loss: 0.6844 Explore P: 0.3416\n",
    "Episode: 124 Total reward: 20.166915893554688 Training loss: 0.2775 Explore P: 0.3389\n",
    "Episode: 125 Total reward: -87.4755859375 Training loss: 0.3127 Explore P: 0.3364\n",
    "Model Saved\n",
    "Episode: 126 Total reward: -17.0537109375 Training loss: 0.3796 Explore P: 0.3337\n",
    "Episode: 127 Total reward: 5.201812744140625 Training loss: 0.6150 Explore P: 0.3311\n",
    "Episode: 128 Total reward: -32.572784423828125 Training loss: 0.2595 Explore P: 0.3285\n",
    "Episode: 129 Total reward: -43.18853759765625 Training loss: 0.4992 Explore P: 0.3259\n",
    "Episode: 130 Total reward: -84.01849365234375 Training loss: 0.3338 Explore P: 0.3226\n",
    "Model Saved\n",
    "Episode: 131 Total reward: -99.23286437988281 Training loss: 1.2294 Explore P: 0.3200\n",
    "Episode: 132 Total reward: -27.938064575195312 Training loss: 0.9042 Explore P: 0.3175\n",
    "Episode: 133 Total reward: 2.96868896484375 Training loss: 0.3110 Explore P: 0.3151\n",
    "Episode: 134 Total reward: -49.97503662109375 Training loss: 0.4291 Explore P: 0.3119\n",
    "Episode: 135 Total reward: 8.848037719726562 Training loss: 0.9113 Explore P: 0.3095\n",
    "Model Saved\n",
    "Episode: 136 Total reward: -78.30146789550781 Training loss: 1.1113 Explore P: 0.3064\n",
    "Episode: 137 Total reward: -35.61848449707031 Training loss: 0.2758 Explore P: 0.3039\n",
    "Episode: 138 Total reward: -80.23164367675781 Training loss: 1.1325 Explore P: 0.3015\n",
    "Episode: 139 Total reward: -41.44696044921875 Training loss: 0.2293 Explore P: 0.2993\n",
    "Episode: 140 Total reward: -63.55998229980469 Training loss: 0.5988 Explore P: 0.2969\n",
    "Model Saved\n",
    "Episode: 141 Total reward: -74.58718872070312 Training loss: 0.3622 Explore P: 0.2956\n",
    "Episode: 142 Total reward: -44.1854248046875 Training loss: 0.8818 Explore P: 0.2933\n",
    "Episode: 143 Total reward: -43.17417907714844 Training loss: 0.6441 Explore P: 0.2918\n",
    "Episode: 144 Total reward: -35.05082702636719 Training loss: 0.1932 Explore P: 0.2885\n",
    "Episode: 145 Total reward: 2.6080322265625 Training loss: 0.2974 Explore P: 0.2857\n",
    "Model Saved\n",
    "Episode: 146 Total reward: -75.66334533691406 Training loss: 0.2797 Explore P: 0.2828\n",
    "Episode: 147 Total reward: -79.89767456054688 Training loss: 14.5457 Explore P: 0.2805\n",
    "Episode: 148 Total reward: -65.21456909179688 Training loss: 0.7638 Explore P: 0.2783\n",
    "Episode: 149 Total reward: 13.195510864257812 Training loss: 0.3936 Explore P: 0.2761\n",
    "Episode: 150 Total reward: 60.77146911621094 Training loss: 1.1485 Explore P: 0.2739\n",
    "Model Saved\n",
    "Episode: 151 Total reward: -67.01502990722656 Training loss: 1.1541 Explore P: 0.2710\n",
    "Episode: 152 Total reward: 7.119903564453125 Training loss: 0.4257 Explore P: 0.2689\n",
    "Episode: 153 Total reward: 13.754486083984375 Training loss: 0.4931 Explore P: 0.2639\n",
    "Episode: 154 Total reward: -67.7314453125 Training loss: 0.5301 Explore P: 0.2618\n",
    "Episode: 155 Total reward: -61.25654602050781 Training loss: 0.3877 Explore P: 0.2599\n",
    "Model Saved\n",
    "Episode: 156 Total reward: -1.2131805419921875 Training loss: 0.3397 Explore P: 0.2579\n",
    "Episode: 157 Total reward: -26.2254638671875 Training loss: 0.1870 Explore P: 0.2558\n",
    "Episode: 158 Total reward: 71.63455200195312 Training loss: 0.3283 Explore P: 0.2538\n",
    "Episode: 159 Total reward: -41.72747802734375 Training loss: 0.6035 Explore P: 0.2520\n",
    "Episode: 160 Total reward: -75.83839416503906 Training loss: 0.5253 Explore P: 0.2488\n",
    "Model Saved\n",
    "Episode: 161 Total reward: 3.0420074462890625 Training loss: 0.8875 Explore P: 0.2468\n",
    "Episode: 162 Total reward: -21.011383056640625 Training loss: 0.2739 Explore P: 0.2449\n",
    "Episode: 163 Total reward: -19.587127685546875 Training loss: 1.2479 Explore P: 0.2431\n",
    "Episode: 164 Total reward: -53.40458679199219 Training loss: 1.2350 Explore P: 0.2413\n",
    "Episode: 165 Total reward: -59.686767578125 Training loss: 0.4527 Explore P: 0.2395\n",
    "Model Saved\n",
    "Episode: 166 Total reward: -53.43865966796875 Training loss: 12.8202 Explore P: 0.2384\n",
    "Episode: 167 Total reward: 4.73968505859375 Training loss: 0.2532 Explore P: 0.2366\n",
    "Episode: 168 Total reward: -42.3804931640625 Training loss: 0.6826 Explore P: 0.2347\n",
    "Episode: 169 Total reward: -1.4572296142578125 Training loss: 0.5197 Explore P: 0.2329\n",
    "Episode: 170 Total reward: -39.27558898925781 Training loss: 11.5407 Explore P: 0.2311\n",
    "Model Saved\n",
    "Episode: 171 Total reward: 8.362579345703125 Training loss: 0.2713 Explore P: 0.2295\n",
    "Episode: 172 Total reward: 14.519943237304688 Training loss: 7.7963 Explore P: 0.2277\n",
    "Episode: 173 Total reward: -58.884429931640625 Training loss: 0.3072 Explore P: 0.2259\n",
    "Episode: 174 Total reward: -93.07179260253906 Training loss: 0.6735 Explore P: 0.2235\n",
    "Episode: 175 Total reward: -60.440277099609375 Training loss: 0.6426 Explore P: 0.2218\n",
    "Model Saved\n",
    "Episode: 176 Total reward: 24.163375854492188 Training loss: 0.5932 Explore P: 0.2201\n",
    "Episode: 177 Total reward: -74.15121459960938 Training loss: 0.1940 Explore P: 0.2191\n",
    "Episode: 178 Total reward: -47.54103088378906 Training loss: 0.9826 Explore P: 0.2174\n",
    "Episode: 179 Total reward: -88.96371459960938 Training loss: 0.6407 Explore P: 0.2157\n",
    "Episode: 180 Total reward: 86.02571105957031 Training loss: 0.3157 Explore P: 0.2134\n",
    "Model Saved\n",
    "Episode: 181 Total reward: -8.269500732421875 Training loss: 1.0492 Explore P: 0.2118\n",
    "Episode: 182 Total reward: 37.916839599609375 Training loss: 0.3531 Explore P: 0.2102\n",
    "Episode: 183 Total reward: 28.824462890625 Training loss: 0.3685 Explore P: 0.2086\n",
    "Episode: 184 Total reward: -103.504150390625 Training loss: 0.8678 Explore P: 0.2077\n",
    "Episode: 185 Total reward: -33.638336181640625 Training loss: 0.5436 Explore P: 0.2062\n",
    "Model Saved\n",
    "Episode: 186 Total reward: -46.80809020996094 Training loss: 0.8421 Explore P: 0.2046\n",
    "\n",
    "Episode: 187 Total reward: 4.5064849853515625 Training loss: 0.2865 Explore P: 0.2030\n",
    "Episode: 188 Total reward: -10.029891967773438 Training loss: 0.4644 Explore P: 0.2014\n",
    "Episode: 189 Total reward: -35.31138610839844 Training loss: 0.3323 Explore P: 0.1999\n",
    "Episode: 190 Total reward: 22.30352783203125 Training loss: 0.6971 Explore P: 0.1984\n",
    "Model Saved\n",
    "Episode: 191 Total reward: -54.252655029296875 Training loss: 0.7283 Explore P: 0.1968\n",
    "Episode: 192 Total reward: -94.67848205566406 Training loss: 1.4658 Explore P: 0.1959\n",
    "Episode: 193 Total reward: -38.33479309082031 Training loss: 0.2945 Explore P: 0.1944\n",
    "Episode: 194 Total reward: -96.05851745605469 Training loss: 0.2530 Explore P: 0.1929\n",
    "Episode: 195 Total reward: -16.951339721679688 Training loss: 0.8220 Explore P: 0.1914\n",
    "Model Saved\n",
    "Episode: 196 Total reward: -104.72447204589844 Training loss: 0.4501 Explore P: 0.1900\n",
    "Episode: 197 Total reward: -3.453094482421875 Training loss: 0.4974 Explore P: 0.1886\n",
    "Episode: 198 Total reward: -26.187362670898438 Training loss: 0.2195 Explore P: 0.1872\n",
    "Episode: 199 Total reward: -98.55648803710938 Training loss: 0.2501 Explore P: 0.1864\n",
    "Episode: 200 Total reward: -16.166595458984375 Training loss: 0.3163 Explore P: 0.1850\n",
    "Model Saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OK6CiNMhFQ41"
   },
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODM3mDAzFQ43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Score:  72.09480285644531\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config(\"/home/schubert/anaconda3/envs/DL/lib/python3.6/site-packages/vizdoom/scenarios/deadly_corridor.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"/home/schubert/anaconda3/envs/DL/lib/python3.6/site-packages/vizdoom/scenarios/deadly_corridor.wad\")\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(1):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            #print(f\"Action:{action} | Reward:{reward}\")\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
